{
	"name": "commonTransforms",
	"properties": {
		"description": "A collection of reusable Python classes that extends out of box PySpark capabilities",
		"folder": {
			"name": "common"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "mediumMO",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4e983031-73bb-40b3-9422-56e55e9b94d7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e836675f-2508-4873-ad4f-754d70253b22/resourceGroups/rg-synapse-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapse01-kn3acb6lw3vr4/bigDataPools/mediumMO",
				"name": "mediumMO",
				"type": "Spark",
				"endpoint": "https://ba-synapse01-kn3acb6lw3vr4.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mediumMO",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# What is CommonTransforms and how to use them in your notebooks ?  \r\n",
					"CommonTransforms is a Python class that uses PySpark libraries to apply common transformations to a Spark dataframe. https://github.com/bennyaustin/pyspark-utils/blob/main/CommonTransforms/README.md"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# CommonTransforms Class"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import trim,when,isnull,lit,col,from_utc_timestamp,to_utc_timestamp,concat_ws,sha1,length,substring,lit,concat,date_add,expr,year,datediff\r\n",
					"from pyspark.sql import functions as F \r\n",
					"import datetime"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class CommonTransforms:\r\n",
					"  inputDf=None\r\n",
					"  inputSchema=None\r\n",
					"  inputColums=None\r\n",
					"  \r\n",
					"#   Constructor\r\n",
					"  def __init__(self, input):\r\n",
					"    self.inputDf=input\r\n",
					"    self.inputSchema=self.inputDf.schema\r\n",
					"    self.inputColumns=self.inputDf.schema.fieldNames()\r\n",
					"    \r\n",
					"#  Remove Leading and Trailing Spaces \r\n",
					"  def trim(self):\r\n",
					"    stringCol= (col for col in self.inputSchema if str(col.dataType)==\"StringType\")\r\n",
					"    for col in stringCol:\r\n",
					"        self.inputDf = self.inputDf.withColumn(col.name,trim(col.name))\r\n",
					"    return self.inputDf\r\n",
					"  \r\n",
					"#   Replace Null values with Default values based on datatypes\r\n",
					"  def replaceNull(self,value, subset=None):\r\n",
					"    isDate=False\r\n",
					"    isTimestamp =False\r\n",
					"    \r\n",
					"    try:\r\n",
					"      if isinstance(value, str):\r\n",
					"        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%d\") #YYYY-MM-DD format e.g \"2020-10-01\"\r\n",
					"        isDate= True\r\n",
					"    except ValueError:\r\n",
					"      isDate=False\r\n",
					"      \r\n",
					"    try:\r\n",
					"      if isinstance(value, str):\r\n",
					"        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\") #YYYY-MM-DDThh:mm:ss format e.g \"2020-10-01T19:50:06\"\r\n",
					"        isTimestamp= True\r\n",
					"    except ValueError:\r\n",
					"      isTimestamp=False\r\n",
					"      \r\n",
					"    if isDate and subset is not None:\r\n",
					"      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True and x.name in subset)\r\n",
					"      for x in dateCol:\r\n",
					"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
					"    elif isDate and subset is None:\r\n",
					"      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True)\r\n",
					"      for x in dateCol:\r\n",
					"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
					"    elif isTimestamp and subset is not None:\r\n",
					"      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True and x.name in subset)\r\n",
					"      for x in tsCol:\r\n",
					"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
					"    elif isTimestamp and subset is None:\r\n",
					"      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True)\r\n",
					"      for x in tsCol:\r\n",
					"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))        \r\n",
					"    else:\r\n",
					"      self.inputDf = self.inputDf.fillna(value,subset)\r\n",
					"      \r\n",
					"    return self.inputDf\r\n",
					"\r\n",
					"#  Remove duplicates\r\n",
					"  def deDuplicate(self, subset=None):\r\n",
					"    self.inputDf = self.inputDf.dropDuplicates(subset)\r\n",
					"    return self.inputDf\r\n",
					"  \r\n",
					"#   Convert UTC timestamp to local\r\n",
					"  def utc_to_local(self,localTimeZone,subset=None):\r\n",
					"    if subset is not None:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
					"    else:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
					"      \r\n",
					"    for x in tsCol:\r\n",
					"      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),localTimeZone))\r\n",
					"    return self.inputDf\r\n",
					"\r\n",
					"#   Convert timestamp in local timezone to UTC\r\n",
					"  def local_to_utc(self,localTimeZone,subset=None):\r\n",
					"    if subset is not None:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
					"    else:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
					"      \r\n",
					"    for x in tsCol:\r\n",
					"      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),localTimeZone))\r\n",
					"    return self.inputDf\r\n",
					"  \r\n",
					"#   Change Timezone\r\n",
					"  def changeTimezone(self,fromTimezone,toTimezone,subset=None):\r\n",
					"    if subset is not None:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
					"    else:\r\n",
					"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
					"    \r\n",
					"    for x in tsCol:\r\n",
					"      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),fromTimezone))\r\n",
					"      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),toTimezone))\r\n",
					"    return self.inputDf\r\n",
					"\r\n",
					"#   Drop System/Non-Business Columns\r\n",
					"  def dropSysColumns(self,columns):\r\n",
					"    self.inputDf = self.inputDf.drop(columns)\r\n",
					"    return self.inputDf \r\n",
					"  \r\n",
					"#  Create Checksum Column \r\n",
					"  def addChecksumCol(self,colName):\r\n",
					"    self.inputDf = self.inputDf.withColumn(colName,sha1(concat_ws(\"~~\", *self.inputDf.columns)))\r\n",
					"    return self.inputDf\r\n",
					"\r\n",
					"# Convert Julian Date to Calendar Date  \r\n",
					"  def julian_to_calendar(self,subset):\r\n",
					"    julCol = (x for x in self.inputSchema if str(x.dataType)==\"IntegerType\" and x.name in subset)\r\n",
					"    for x in julCol:\r\n",
					"      self.inputDf = (self.inputDf.withColumn(x.name,col(x.name).cast(\"string\"))\r\n",
					"                                 .withColumn(x.name+\"_year\",\r\n",
					"                                             when((length(col(x.name))==5) & (substring(col(x.name),1,2) <=50),concat(lit('20'),substring(col(x.name),1,2)))\r\n",
					"                                             .when((length(col(x.name))==5) & (substring(col(x.name),1,2) >50),concat(lit('19'),substring(col(x.name),1,2)))\r\n",
					"                                             .when(length(col(x.name))==7,substring(col(x.name),1,4))\r\n",
					"                                             .otherwise(lit(0))\r\n",
					"                                            )\r\n",
					"                                 .withColumn(x.name+\"_days\",\r\n",
					"                                             when(length(col(x.name))==5,substring(col(x.name),3,3).cast(\"int\"))\r\n",
					"                                             .when(length(col(x.name))==7,substring(col(x.name),5,3).cast(\"int\"))\r\n",
					"                                             .otherwise(lit(0))\r\n",
					"                                            )\r\n",
					"                                 .withColumn(x.name+\"_ref_year\",concat(col(x.name+\"_year\"),lit(\"-01\"),lit(\"-01\")).cast(\"date\"))\r\n",
					"                                 .withColumn(x.name+\"_calendar\",expr(\"date_add(\" + x.name+\"_ref_year\"+\",\"+ x.name+\"_days)-1\"))\r\n",
					"                                 .drop(x.name, x.name+\"_year\",x.name+\"_days\",x.name+\"_ref_year\")\r\n",
					"                                 .withColumnRenamed(x.name+\"_calendar\",x.name)\r\n",
					"                                 \r\n",
					"                     )\r\n",
					"    return self.inputDf \r\n",
					"  \r\n",
					"# Convert Calendar Date to Julian Date \r\n",
					"  def calendar_to_julian(self, subset):\r\n",
					"    calCol = (x for x in self.inputSchema if ((str(x.dataType)==\"DateType\" or str(x.dataType)==\"TimestampType\") and x.name in subset))\r\n",
					"\r\n",
					"    for x in calCol:\r\n",
					"      self.inputDf = (self.inputDf.withColumn(x.name+\"_ref_year\", concat(year(col(x.name)).cast(\"string\"),lit(\"-01\"),lit(\"-01\")))\r\n",
					"                                  .withColumn(x.name+\"_datediff\", datediff(col(x.name),col(x.name+\"_ref_year\"))+1)\r\n",
					"                                  .withColumn(x.name+\"_julian\", concat(substring(year(col(x.name)).cast(\"string\"),3,2),col(x.name+\"_datediff\")).cast(\"int\"))\r\n",
					"                                  .drop(x.name,x.name+\"_ref_year\",x.name+\"_datediff\")\r\n",
					"                                  .withColumnRenamed(x.name+\"_julian\",x.name)\r\n",
					"                     )\r\n",
					"    return self.inputDf\r\n",
					"\r\n",
					"# Add a set of literal value columns to dataframe, pass as dictionary parameter  \r\n",
					"  def addLitCols(self,colDict):\r\n",
					"    for x in colDict.items():\r\n",
					"      self.inputDf = self.inputDf.withColumn(x[0],lit(x[1]))\r\n",
					"    return self.inputDf"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def flattenNested(df):\r\n",
					"    ###########################################################################################################################  \r\n",
					"    # Function: flattenNested\r\n",
					"    # Returns a flattened version of XML, JSON\r\n",
					"    #\r\n",
					"    # Parameters:\r\n",
					"    # df = Dataframe loaded with the Nested XML or JSON structure\r\n",
					"    #\r\n",
					"    # Returns:\r\n",
					"    # The dataframe with the flattened version of the XML or JSON structure\r\n",
					"    ##########################################################################################################################      \r\n",
					"    structCols = []\r\n",
					"    sep = \"_\"\r\n",
					"\r\n",
					"    for colName, colType in df.dtypes:\r\n",
					"        if colType.startswith(\"struct\"):\r\n",
					"            structCols.append(colName)\r\n",
					"\r\n",
					"    arrayCols = []\r\n",
					"    for colName, colType in df.dtypes:\r\n",
					"        if colType.startswith(\"array\"):\r\n",
					"            arrayCols.append(colName)\r\n",
					"\r\n",
					"    if structCols:  \r\n",
					"        structElement = []\r\n",
					"        for colName, colType in df.dtypes:\r\n",
					"            if colType.startswith(\"struct\"):\r\n",
					"                structElement.append(colName)\r\n",
					"        flattenCols = [fc for fc, _ in df.dtypes if fc not in structElement]\r\n",
					"        for nc in structElement:\r\n",
					"            for cc in df.select(f\"{nc}.*\").columns:\r\n",
					"                 flattenCols.append(F.col(f\"{nc}.{cc}\").alias(f\"{nc}{sep}{cc}\"))\r\n",
					"\r\n",
					"        df = df.select(flattenCols)          \r\n",
					"        return FlattenNested(df)\r\n",
					"\r\n",
					"    if arrayCols:\r\n",
					"        arrayElement = []\r\n",
					"        for colName, colType in df.dtypes:\r\n",
					"            if colType.startswith(\"array\"):\r\n",
					"                arrayElement.append(colName)\r\n",
					"\r\n",
					"        explodeddf = df\r\n",
					"        for nc in arrayElement:\r\n",
					"            explodeddf = explodeddf.withColumn(nc, F.explode_outer(F.col(nc)))     \r\n",
					"\r\n",
					"        df = explodeddf \r\n",
					"        return FlattenNested(df) \r\n",
					"\r\n",
					"    return df    "
				],
				"execution_count": 3
			}
		]
	}
}