{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ba-synapseanalytics01"
		},
		"ControlDB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ControlDB'"
		},
		"SampleDB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SampleDB'"
		},
		"ba-synapseanalytics01-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ba-synapseanalytics01-WorkspaceDefaultSqlServer'"
		},
		"AtlasREST_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://ba-purview02-aug-pubpreview.purview.azure.com"
		},
		"AtlasREST_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "5e07b142-92b4-4671-83c0-e824bc93da6c"
		},
		"AtlasREST_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AtlasREST_properties_typeProperties_aadResourceId": {
			"type": "string",
			"defaultValue": "https://purview.azure.net"
		},
		"Bronze_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bastoragedatalake01.dfs.core.windows.net/"
		},
		"REST_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().BaseURL}"
		},
		"REST_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "@{linkedService().ServicePrincipalID}"
		},
		"REST_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "@{linkedService().TenantID}"
		},
		"REST_properties_typeProperties_aadResourceId": {
			"type": "string",
			"defaultValue": "@{linkedService().AADResource}"
		},
		"ba-synapseanalytics01-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bastoragedatalake01.dfs.core.windows.net"
		},
		"bamachinelearningws_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d"
		},
		"bamachinelearningws_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "rg-dataplatform"
		},
		"bamachinelearningws_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "118aab7c-6863-4684-9252-f3e48e747bd5"
		},
		"bamachinelearningws_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"keyvault01_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://ba-keyvault1.vault.azure.net/"
		},
		"storagecosmosdb1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagecosmosdb1.dfs.core.windows.net"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_SourceSystemName": {
			"type": "string",
			"defaultValue": "Azure"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_StreamName": {
			"type": "string",
			"defaultValue": "%"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_MaxIngestInstance": {
			"type": "string",
			"defaultValue": "100"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_BaseURL": {
			"type": "string",
			"defaultValue": "https://management.azure.com"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_ServicePrincipalID": {
			"type": "string",
			"defaultValue": "5e07b142-92b4-4671-83c0-e824bc93da6c"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_TenantID": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Azure Daily_properties_Master ELT REST API_parameters_AADResource": {
			"type": "string",
			"defaultValue": "https://management.azure.com"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_resourceList_logicapp_endpoint": {
			"type": "string",
			"defaultValue": "https://prod-14.australiaeast.logic.azure.com:443/workflows/3938e046963c48968a48636f821d6970/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=HcQMejUIYn77Y9X6Relz4RY31AXZEEyA54qfNulmhFo"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_subscriptionId": {
			"type": "string",
			"defaultValue": "5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_resourceType": {
			"type": "string",
			"defaultValue": "Microsoft.Synapse/workspaces/sqlPools"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_apiVersion": {
			"type": "string",
			"defaultValue": "2021-04-01"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_action": {
			"type": "string",
			"defaultValue": "pause"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_sku": {
			"type": "string",
			"defaultValue": "DW100c"
		},
		"Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_sqlPool_logicapp_endpoint": {
			"type": "string",
			"defaultValue": "https://prod-19.australiaeast.logic.azure.com:443/workflows/5f8598c0da594852a579269eadaf38e2/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=OkMprjcH_d3xprJfXXr5iZkZN8-IVIewsLg6GLLSaos"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_SourceSystemName": {
			"type": "string",
			"defaultValue": "Purview"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_StreamName": {
			"type": "string",
			"defaultValue": "%"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_MaxIngestInstance": {
			"type": "string",
			"defaultValue": "50"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_BaseURL": {
			"type": "string",
			"defaultValue": "https://ba-purview02-aug-pubpreview.purview.azure.com"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_ServicePrincipalID": {
			"type": "string",
			"defaultValue": "5e07b142-92b4-4671-83c0-e824bc93da6c"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_TenantID": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Purview Daily_properties_Master ELT REST API_parameters_AADResource": {
			"type": "string",
			"defaultValue": "https://purview.azure.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Copy from REST API')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CopyREST2Parquet",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "FlattenHierarchy"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"path": "['name']"
										},
										"sink": {
											"name": "name",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['friendlyName']"
										},
										"sink": {
											"name": "friendlyName",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['description']"
										},
										"sink": {
											"name": "description",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['systemData']['createdBy']"
										},
										"sink": {
											"name": "createdBy",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['systemData']['createdByType']"
										},
										"sink": {
											"name": "createdByType",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['systemData']['createdAt']"
										},
										"sink": {
											"name": "createdAt",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"path": "['systemData']['lastModifiedByType']"
										},
										"sink": {
											"name": "lastModifiedByType",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['systemData']['lastModifiedAt']"
										},
										"sink": {
											"name": "lastModifiedAt",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"path": "['collectionProvisioningState']"
										},
										"sink": {
											"name": "collectionProvisioningState",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['parentCollection']['type']"
										},
										"sink": {
											"name": "parentCollectionType",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "['parentCollection']['referenceName']"
										},
										"sink": {
											"name": "parentCollectionReferenceName",
											"type": "String"
										}
									}
								],
								"collectionReference": "$['value']",
								"mapComplexValuesToString": true
							}
						},
						"inputs": [
							{
								"referenceName": "AtlasREST_API",
								"type": "DatasetReference",
								"parameters": {
									"RelativeUrl": {
										"value": "@pipeline().parameters.EntityName",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Bronze_Parquet",
								"type": "DatasetReference",
								"parameters": {
									"FileSystem": {
										"value": "@pipeline().parameters.DestinationRawFileSystem",
										"type": "Expression"
									},
									"Folder": {
										"value": "@pipeline().parameters.DestinationRawFolder",
										"type": "Expression"
									},
									"File": {
										"value": "@pipeline().parameters.DestinationRawFile",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"EntityName": {
						"type": "string",
						"defaultValue": "/collections?api-version=2019-11-01-preview"
					},
					"DestinationRawFileSystem": {
						"type": "string",
						"defaultValue": "raw-bronze"
					},
					"DestinationRawFolder": {
						"type": "string",
						"defaultValue": "purview/collections/2022-06"
					},
					"DestinationRawFile": {
						"type": "string",
						"defaultValue": "collections_20220625.parquet"
					}
				},
				"folder": {
					"name": "Sandbox"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AtlasREST_API')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy using Parametrized DataMapping')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CopyREST2Parquet",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set datamapping",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "FlattenHierarchy"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"value": "@json(variables('datamapping'))",
								"type": "Expression"
							}
						},
						"inputs": [
							{
								"referenceName": "AtlasREST_API",
								"type": "DatasetReference",
								"parameters": {
									"RelativeUrl": {
										"value": "@pipeline().parameters.EntityName",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Bronze_Parquet",
								"type": "DatasetReference",
								"parameters": {
									"FileSystem": {
										"value": "@pipeline().parameters.DestinationRawFileSystem",
										"type": "Expression"
									},
									"Folder": {
										"value": "@pipeline().parameters.DestinationRawFolder",
										"type": "Expression"
									},
									"File": {
										"value": "@pipeline().parameters.DestinationRawFile",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Set datamapping",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "datamapping",
							"value": " {\n    \"type\": \"TabularTranslator\",\n    \"mappings\": [\n        {\n            \"source\": {\n                \"path\": \"['name']\"\n            },\n            \"sink\": {\n                \"name\": \"name\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['friendlyName']\"\n            },\n            \"sink\": {\n                \"name\": \"friendlyName\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['description']\"\n            },\n            \"sink\": {\n                \"name\": \"description\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['systemData']['createdBy']\"\n            },\n            \"sink\": {\n                \"name\": \"createdBy\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['systemData']['createdByType']\"\n            },\n            \"sink\": {\n                \"name\": \"createdByType\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['systemData']['createdAt']\"\n            },\n            \"sink\": {\n                \"name\": \"createdAt\",\n                \"type\": \"DateTime\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['systemData']['lastModifiedByType']\"\n            },\n            \"sink\": {\n                \"name\": \"lastModifiedByType\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['systemData']['lastModifiedAt']\"\n            },\n            \"sink\": {\n                \"name\": \"lastModifiedAt\",\n                \"type\": \"DateTime\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['collectionProvisioningState']\"\n            },\n            \"sink\": {\n                \"name\": \"collectionProvisioningState\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['parentCollection']['type']\"\n            },\n            \"sink\": {\n                \"name\": \"parentCollectionType\",\n                \"type\": \"String\"\n            }\n        },\n        {\n            \"source\": {\n                \"path\": \"['parentCollection']['referenceName']\"\n            },\n            \"sink\": {\n                \"name\": \"parentCollectionReferenceName\",\n                \"type\": \"String\"\n            }\n        }\n    ],\n    \"collectionReference\": \"$['value']\",\n    \"mapComplexValuesToString\": true\n}"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"EntityName": {
						"type": "string",
						"defaultValue": "collections?api-version=2019-11-01-preview"
					},
					"DestinationRawFileSystem": {
						"type": "string",
						"defaultValue": "raw-bronze"
					},
					"DestinationRawFolder": {
						"type": "string",
						"defaultValue": "purview/collections/2022-06"
					},
					"DestinationRawFile": {
						"type": "string",
						"defaultValue": "collections_20220625.parquet"
					}
				},
				"variables": {
					"datamapping": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Sandbox"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AtlasREST_API')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Bearer Auth')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Bearer Token",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "application/x-www-form-urlencoded"
							},
							"body": {
								"value": "tenant=@{pipeline().parameters.TenantID}&client_id=@{pipeline().parameters.ClientID}&client_secret=@{pipeline().parameters.ClientSecret}&grant_type=client_credentials&scope=https://purview.azure.net",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set BearerToken",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get Bearer Token",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "BearerToken",
							"value": {
								"value": "@activity('Get Bearer Token').output.access_token",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"TenantID": {
						"type": "string",
						"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
					},
					"ClientID": {
						"type": "string",
						"defaultValue": "5e07b142-92b4-4671-83c0-e824bc93da6c"
					},
					"ClientSecret": {
						"type": "string",
						"defaultValue": "nzh7cA-~MBl0OA9_rQK62-i4r14e_tZ_j-"
					}
				},
				"variables": {
					"BearerToken": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Sandbox"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IngestREST')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Generic Pipeline to ingest data from any REST API",
				"activities": [
					{
						"name": "Ingest Running",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "ADFPipelineRunID",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[ELT].[InsertIngestInstance]",
							"storedProcedureParameters": {
								"ADFPipelineRunID": {
									"value": {
										"value": "@variables('ADFPipelineRunID')",
										"type": "Expression"
									},
									"type": "Guid"
								},
								"DestinationRawFile": {
									"value": {
										"value": "@pipeline().parameters.DestinationRawFile",
										"type": "Expression"
									},
									"type": "String"
								},
								"DestinationRawFileSystem": {
									"value": {
										"value": "@pipeline().parameters.DestinationRawFileSystem",
										"type": "Expression"
									},
									"type": "String"
								},
								"DestinationRawFolder": {
									"value": {
										"value": "@pipeline().parameters.DestinationRawFolder",
										"type": "Expression"
									},
									"type": "String"
								},
								"IngestID": {
									"value": {
										"value": "@pipeline().parameters.IngestID",
										"type": "Expression"
									},
									"type": "Int32"
								},
								"ReloadFlag": {
									"value": {
										"value": "@pipeline().parameters.ReloadFlag",
										"type": "Expression"
									},
									"type": "Boolean"
								},
								"SourceFileDropFile": {
									"value": null,
									"type": "String"
								},
								"SourceFileDropFileSystem": {
									"value": null,
									"type": "String"
								},
								"SourceFileDropFolder": {
									"value": null,
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "ControlDB",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Source To Datalake Bronze Zone",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Ingest Running",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"value": "@json(pipeline().parameters.DataMapping)",
								"type": "Expression"
							}
						},
						"inputs": [
							{
								"referenceName": "REST_API",
								"type": "DatasetReference",
								"parameters": {
									"BaseURL": {
										"value": "@pipeline().parameters.BaseURL",
										"type": "Expression"
									},
									"ServicePrincipalID": {
										"value": "@pipeline().parameters.ServicePrincipalID",
										"type": "Expression"
									},
									"TenantID": {
										"value": "@pipeline().parameters.TenantID",
										"type": "Expression"
									},
									"AADResource": {
										"value": "@pipeline().parameters.AADResource",
										"type": "Expression"
									},
									"RelativeURL": {
										"value": "@pipeline().parameters.SourceSQL",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Bronze_Parquet",
								"type": "DatasetReference",
								"parameters": {
									"FileSystem": {
										"value": "@pipeline().parameters.DestinationRawFileSystem",
										"type": "Expression"
									},
									"Folder": {
										"value": "@pipeline().parameters.DestinationRawFolder",
										"type": "Expression"
									},
									"File": {
										"value": "@pipeline().parameters.DestinationRawFile",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "ADFPipelineRunID",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "ADFPipelineRunID",
							"value": {
								"value": "@{if(empty(pipeline().parameters.ADFPipelineRunID),pipeline().RunId,pipeline().parameters.ADFPipelineRunID)}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Ingest Success",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Source To Datalake Bronze Zone",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[ELT].[UpdateIngestInstance]",
							"storedProcedureParameters": {
								"ADFIngestPipelineRunID": {
									"value": {
										"value": "@variables('ADFPipelineRunID')",
										"type": "Expression"
									},
									"type": "Guid"
								},
								"DataFromNumber": {
									"value": null,
									"type": "Int32"
								},
								"DataFromTimestamp": {
									"value": null,
									"type": "DateTime"
								},
								"DataToNumber": {
									"value": null,
									"type": "Int32"
								},
								"DataToTimestamp": {
									"value": null,
									"type": "DateTime"
								},
								"IngestCount": {
									"value": {
										"value": "@activity('Source To Datalake Bronze Zone').output.rowsCopied",
										"type": "Expression"
									},
									"type": "Int32"
								},
								"IngestStatus": {
									"value": "Success",
									"type": "String"
								},
								"ReloadFlag": {
									"value": {
										"value": "@pipeline().parameters.ReloadFlag",
										"type": "Expression"
									},
									"type": "Boolean"
								},
								"SourceCount": {
									"value": {
										"value": "@activity('Source To Datalake Bronze Zone').output.rowsRead",
										"type": "Expression"
									},
									"type": "Int32"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "ControlDB",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Ingest Failure",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Source To Datalake Bronze Zone",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[ELT].[UpdateIngestInstance]",
							"storedProcedureParameters": {
								"ADFIngestPipelineRunID": {
									"value": {
										"value": "@variables('ADFPipelineRunID')",
										"type": "Expression"
									},
									"type": "Guid"
								},
								"DataFromNumber": {
									"value": null,
									"type": "Int32"
								},
								"DataFromTimestamp": {
									"value": null,
									"type": "DateTime"
								},
								"DataToNumber": {
									"value": null,
									"type": "Int32"
								},
								"DataToTimestamp": {
									"value": null,
									"type": "DateTime"
								},
								"IngestCount": {
									"value": null,
									"type": "Int32"
								},
								"IngestStatus": {
									"value": "Failure",
									"type": "String"
								},
								"ReloadFlag": {
									"value": {
										"value": "@pipeline().parameters.ReloadFlag",
										"type": "Expression"
									},
									"type": "Boolean"
								},
								"SourceCount": {
									"value": null,
									"type": "Int32"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "ControlDB",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Get L1Transform Definition",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Ingest Success",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderStoredProcedureName": "[ELT].[GetTransformDefinition_L1]",
								"storedProcedureParameters": {
									"DeltaDate": {
										"type": "DateTime",
										"value": {
											"value": "@pipeline().parameters.DataFromTimestamp",
											"type": "Expression"
										}
									},
									"IngestID": {
										"type": "Int32",
										"value": {
											"value": "@pipeline().parameters.IngestID",
											"type": "Expression"
										}
									}
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "ControlDB_Dataset",
								"type": "DatasetReference",
								"parameters": {
									"Table": "NotApplicable",
									"Schema": "NotApplicable"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Get L2Transform Definition",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach L1Transform",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderStoredProcedureName": "[ELT].[GetTransformDefinition_L2]",
								"storedProcedureParameters": {
									"DeltaDate": {
										"type": "DateTime",
										"value": {
											"value": "@pipeline().parameters.DataFromTimestamp",
											"type": "Expression"
										}
									},
									"IngestID": {
										"type": "Int32",
										"value": {
											"value": "@pipeline().parameters.IngestID",
											"type": "Expression"
										}
									},
									"InputType": {
										"type": "String",
										"value": null
									}
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "ControlDB_Dataset",
								"type": "DatasetReference",
								"parameters": {
									"Table": "NotApplicable",
									"Schema": "NotApplicable"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach L2Transform",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get L2Transform Definition",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get L2Transform Definition').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Insert L2Transform Instance",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"storedProcedureName": "[ELT].[InsertTransformInstance_L2]",
										"storedProcedureParameters": {
											"CustomParameters": {
												"value": {
													"value": "@item().CustomParameters",
													"type": "Expression"
												},
												"type": "String"
											},
											"DataFromNumber": {
												"value": {
													"value": "@item().DataFromNumber",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"DataFromTimestamp": {
												"value": {
													"value": "@item().DataToTimestamp",
													"type": "Expression"
												},
												"type": "DateTime"
											},
											"DataToNumber": {
												"value": {
													"value": "@item().DataToNumber",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"DataToTimestamp": {
												"value": {
													"value": "@item().DataToTimestamp",
													"type": "Expression"
												},
												"type": "DateTime"
											},
											"DeltaName": {
												"value": {
													"value": "@item().DeltaName",
													"type": "Expression"
												},
												"type": "String"
											},
											"IngestADFPipelineRunID": {
												"value": {
													"value": "@variables('ADFPipelineRunID')",
													"type": "Expression"
												},
												"type": "Guid"
											},
											"IngestID": {
												"value": {
													"value": "@item().IngestID",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"InputDWTable": {
												"value": {
													"value": "@item().InputDWTable",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputFile": {
												"value": {
													"value": "@item().InputFile",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputFileDelimiter": {
												"value": {
													"value": "@item().InputFileDelimiter",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputFileFolder": {
												"value": {
													"value": "@item().InputFileFolder",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputFileHeaderFlag": {
												"value": {
													"value": "@item().InputFileHeaderFlag",
													"type": "Expression"
												},
												"type": "Boolean"
											},
											"InputFileSystem": {
												"value": {
													"value": "@item().InputFileSystem",
													"type": "Expression"
												},
												"type": "String"
											},
											"L1TransformADFPipelineRunID": {
												"value": null,
												"type": "Guid"
											},
											"L1TransformID": {
												"value": {
													"value": "@item().L1TransformID",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"L2TransformID": {
												"value": {
													"value": "@item().L2TransformID",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"LastDeltaDate": {
												"value": null,
												"type": "DateTime"
											},
											"LastDeltaNumber": {
												"value": null,
												"type": "Int32"
											},
											"LookupColumns": {
												"value": {
													"value": "@item().LookupColumns",
													"type": "Expression"
												},
												"type": "String"
											},
											"MaxRetries": {
												"value": {
													"value": "@item().MaxRetries",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"NotebookName": {
												"value": {
													"value": "@item().NotebookName",
													"type": "Expression"
												},
												"type": "String"
											},
											"NotebookPath": {
												"value": {
													"value": "@item().NotebookPath",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWStagingTable": {
												"value": {
													"value": "@item().OutputDWStagingTable",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWTable": {
												"value": {
													"value": "@item().OutputDWTable",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWTableWriteMode": {
												"value": {
													"value": "@item().OutputDWTableWriteMode",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CuratedFile": {
												"value": {
													"value": "@item().OutputL2CuratedFile",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CuratedFileDelimiter": {
												"value": {
													"value": "@item().OutputL2CuratedFileDelimiter",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CuratedFileFormat": {
												"value": {
													"value": "@item().OutputL2CuratedFileFormat",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CuratedFileWriteMode": {
												"value": {
													"value": "@item().OutputL2CuratedFileWriteMode",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CuratedFolder": {
												"value": {
													"value": "@item().OutputL2CuratedFolder",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL2CurateFileSystem": {
												"value": {
													"value": "@item().OutputL2CurateFileSystem",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "ControlDB",
										"type": "LinkedServiceReference"
									}
								}
							]
						}
					},
					{
						"name": "ForEach L1Transform",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get L1Transform Definition",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get L1Transform Definition').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Insert L1Transform Instance",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"storedProcedureName": "[ELT].[InsertTransformInstance_L1]",
										"storedProcedureParameters": {
											"CustomParameters": {
												"value": {
													"value": "@item().CustomParameters",
													"type": "Expression"
												},
												"type": "String"
											},
											"IngestADFPipelineRunID": {
												"value": {
													"value": "@variables('ADFPipelineRunID')",
													"type": "Expression"
												},
												"type": "Guid"
											},
											"IngestCount": {
												"value": {
													"value": "@activity('Source To Datalake Bronze Zone').output.rowsCopied",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"IngestID": {
												"value": {
													"value": "@item().IngestID",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"IngestInstanceID": {
												"value": null,
												"type": "Int32"
											},
											"InputFileHeaderFlag": {
												"value": {
													"value": "@item().InputFileHeaderFlag",
													"type": "Expression"
												},
												"type": "Boolean"
											},
											"InputRawFile": {
												"value": {
													"value": "@pipeline().parameters.DestinationRawFile",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputRawFileDelimiter": {
												"value": {
													"value": "@item().InputRawFileDelimiter",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputRawFileFolder": {
												"value": {
													"value": "@pipeline().parameters.DestinationRawFolder",
													"type": "Expression"
												},
												"type": "String"
											},
											"InputRawFileSystem": {
												"value": {
													"value": "@pipeline().parameters.DestinationRawFileSystem",
													"type": "Expression"
												},
												"type": "String"
											},
											"L1TransformID": {
												"value": {
													"value": "@item().L1TransformID",
													"type": "Expression"
												},
												"type": "Int32"
											},
											"LookupColumns": {
												"value": {
													"value": "@item().LookupColumns",
													"type": "Expression"
												},
												"type": "String"
											},
											"NotebookName": {
												"value": {
													"value": "@item().NotebookName",
													"type": "Expression"
												},
												"type": "String"
											},
											"NotebookPath": {
												"value": {
													"value": "@item().NotebookPath",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWStagingTable": {
												"value": {
													"value": "@item().OutputDWStagingTable",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWTable": {
												"value": {
													"value": "@item().OutputDWTable",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputDWTableWriteMode": {
												"value": {
													"value": "@item().OutputDWTableWriteMode",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CuratedFile": {
												"value": {
													"value": "@item().OutputL1CuratedFile",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CuratedFileDelimiter": {
												"value": {
													"value": "@item().OutputL1CuratedFileDelimiter",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CuratedFileFormat": {
												"value": {
													"value": "@item().OutputL1CuratedFileFormat",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CuratedFileWriteMode": {
												"value": {
													"value": "@item().OutputL1CuratedFileWriteMode",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CuratedFolder": {
												"value": {
													"value": "@item().OutputL1CuratedFolder",
													"type": "Expression"
												},
												"type": "String"
											},
											"OutputL1CurateFileSystem": {
												"value": {
													"value": "@item().OutputL1CurateFileSystem",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "ControlDB",
										"type": "LinkedServiceReference"
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"IngestID": {
						"type": "int"
					},
					"SourceSystemName": {
						"type": "string"
					},
					"StreamName": {
						"type": "string"
					},
					"Backend": {
						"type": "string"
					},
					"EntityName": {
						"type": "string"
					},
					"DeltaName": {
						"type": "string"
					},
					"LastDeltaDate": {
						"type": "string"
					},
					"DataFromTimestamp": {
						"type": "string"
					},
					"DataToTimestamp": {
						"type": "string"
					},
					"LastDeltaNumber": {
						"type": "int"
					},
					"DataFromNumber": {
						"type": "int"
					},
					"DataToNumber": {
						"type": "int"
					},
					"DataFormat": {
						"type": "string"
					},
					"SourceStructure": {
						"type": "string"
					},
					"MaxIntervalMinutes": {
						"type": "int"
					},
					"MaxIntervalNumber": {
						"type": "int"
					},
					"DataMapping": {
						"type": "string"
					},
					"RunSequence": {
						"type": "int"
					},
					"ActiveFlag": {
						"type": "bool"
					},
					"L1TransformationReqdFlag": {
						"type": "string"
					},
					"L2TransformationReqdFlag": {
						"type": "string"
					},
					"DelayL1TransformationFlag": {
						"type": "bool"
					},
					"DelayL2TransformationFlag": {
						"type": "bool"
					},
					"DestinationRawFileSystem": {
						"type": "string"
					},
					"DestinationRawFolder": {
						"type": "string"
					},
					"DestinationRawFile": {
						"type": "string"
					},
					"SourceSQL": {
						"type": "string"
					},
					"StatSQL": {
						"type": "string"
					},
					"ReloadFlag": {
						"type": "bool"
					},
					"ADFPipelineRunID": {
						"type": "string"
					},
					"BaseURL": {
						"type": "string"
					},
					"ServicePrincipalID": {
						"type": "string"
					},
					"TenantID": {
						"type": "string"
					},
					"AADResource": {
						"type": "string"
					}
				},
				"variables": {
					"ADFPipelineRunID": {
						"type": "String"
					}
				},
				"folder": {
					"name": "REST API Pipelines"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ControlDB')]",
				"[concat(variables('workspaceId'), '/datasets/REST_API')]",
				"[concat(variables('workspaceId'), '/datasets/Bronze_Parquet')]",
				"[concat(variables('workspaceId'), '/datasets/ControlDB_Dataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Manage-All-SQLPools')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Call Resource List Logic App",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Set Logic App Parameters",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.resourceList_logicapp_endpoint",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"value": "@json(variables('parameters'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set Logic App Parameters",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "parameters",
							"value": {
								"value": "{\"subscriptionId\": \"@{pipeline().parameters.subscriptionId}\",\n\"resourceType\": \"@{pipeline().parameters.resourceType}\",\n\"apiVersion\": \"@{pipeline().parameters.apiVersion}\"\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach-SQLPool",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Call Resource List Logic App",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Call Resource List Logic App').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "resourceGroupName",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "resourceGroupName",
										"value": {
											"value": "@last(take(split(item().id,'/'),5))",
											"type": "Expression"
										}
									}
								},
								{
									"name": "workspaceName",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "workspaceName",
										"value": {
											"value": "@first(split(item().name,'/'))",
											"type": "Expression"
										}
									}
								},
								{
									"name": "sqlPoolName",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "sqlPoolName",
										"value": {
											"value": "@last(split(item().name,'/'))",
											"type": "Expression"
										}
									}
								},
								{
									"name": "SQLPool-Pause-Resume-Scale",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "resourceGroupName",
											"dependencyConditions": [
												"Succeeded"
											]
										},
										{
											"activity": "workspaceName",
											"dependencyConditions": [
												"Succeeded"
											]
										},
										{
											"activity": "sqlPoolName",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "SQLPool-Pause-Resume-Scale",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"logicapp_endpoint": {
												"value": "@pipeline().parameters.sqlPool_logicapp_endpoint",
												"type": "Expression"
											},
											"subscriptionId": {
												"value": "@pipeline().parameters.subscriptionId",
												"type": "Expression"
											},
											"resourceGroupName": {
												"value": "@variables('resourceGroupName')",
												"type": "Expression"
											},
											"workspaceName": {
												"value": "@variables('workspaceName')",
												"type": "Expression"
											},
											"sqlPoolName": {
												"value": "@variables('sqlPoolName')",
												"type": "Expression"
											},
											"apiVersion": {
												"value": "2021-03-01",
												"type": "Expression"
											},
											"action": {
												"value": "@pipeline().parameters.action",
												"type": "Expression"
											},
											"sku": {
												"value": "@pipeline().parameters.sku",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"resourceList_logicapp_endpoint": {
						"type": "string"
					},
					"subscriptionId": {
						"type": "string"
					},
					"resourceType": {
						"type": "string",
						"defaultValue": "Microsoft.Synapse/workspaces/sqlPools"
					},
					"apiVersion": {
						"type": "string",
						"defaultValue": "2021-04-01"
					},
					"action": {
						"type": "string",
						"defaultValue": "pause"
					},
					"sku": {
						"type": "string",
						"defaultValue": "DW100c"
					},
					"sqlPool_logicapp_endpoint": {
						"type": "string"
					}
				},
				"variables": {
					"parameters": {
						"type": "String"
					},
					"resourceGroupName": {
						"type": "String"
					},
					"workspaceName": {
						"type": "String"
					},
					"sqlPoolName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Manage-SQLPool"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/pipelines/SQLPool-Pause-Resume-Scale')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Master ELT REST API')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Wrapper Pipeline that Extracts, Loads and Transforms data from a REST API data source",
				"activities": [
					{
						"name": "Get Ingest Instances",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderStoredProcedureName": "[ELT].[GetIngestDefinition]",
								"storedProcedureParameters": {
									"MaxIngestInstance": {
										"type": "Int32",
										"value": {
											"value": "@pipeline().parameters.MaxIngestInstance",
											"type": "Expression"
										}
									},
									"SourceSystemName": {
										"type": "String",
										"value": {
											"value": "@pipeline().parameters.SourceSystemName",
											"type": "Expression"
										}
									},
									"StreamName": {
										"type": "String",
										"value": {
											"value": "@pipeline().parameters.StreamName",
											"type": "Expression"
										}
									}
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "ControlDB_Dataset",
								"type": "DatasetReference",
								"parameters": {
									"Table": "NotApplicable",
									"Schema": "NotApplicable"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach Ingest Instance",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Ingest Instances",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Ingest Instances').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "IngestREST",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "IngestREST",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"IngestID": {
												"value": "@item().IngestID",
												"type": "Expression"
											},
											"SourceSystemName": {
												"value": "@item().SourceSystemName",
												"type": "Expression"
											},
											"StreamName": {
												"value": "@item().StreamName",
												"type": "Expression"
											},
											"Backend": {
												"value": "@item().Backend",
												"type": "Expression"
											},
											"EntityName": {
												"value": "@item().EntityName",
												"type": "Expression"
											},
											"DeltaName": {
												"value": "@item().DeltaName",
												"type": "Expression"
											},
											"LastDeltaDate": {
												"value": "@item().LastDeltaDate",
												"type": "Expression"
											},
											"DataFromTimestamp": {
												"value": "@item().DataFromTimestamp",
												"type": "Expression"
											},
											"DataToTimestamp": {
												"value": "@item().DataToTimestamp",
												"type": "Expression"
											},
											"LastDeltaNumber": {
												"value": "@item().LastDeltaNumber",
												"type": "Expression"
											},
											"DataFromNumber": {
												"value": "@item().DataFromNumber",
												"type": "Expression"
											},
											"DataToNumber": {
												"value": "@item().DataToNumber",
												"type": "Expression"
											},
											"DataFormat": {
												"value": "@item().DataFormat",
												"type": "Expression"
											},
											"SourceStructure": {
												"value": "@item().SourceStructure",
												"type": "Expression"
											},
											"MaxIntervalMinutes": {
												"value": "@item().MaxIntervalMinutes",
												"type": "Expression"
											},
											"MaxIntervalNumber": {
												"value": "@item().MaxIntervalNumber",
												"type": "Expression"
											},
											"DataMapping": {
												"value": "@item().DataMapping",
												"type": "Expression"
											},
											"RunSequence": {
												"value": "@item().RunSequence",
												"type": "Expression"
											},
											"ActiveFlag": {
												"value": "@item().ActiveFlag",
												"type": "Expression"
											},
											"L1TransformationReqdFlag": {
												"value": "@item().L1TransformationReqdFlag",
												"type": "Expression"
											},
											"L2TransformationReqdFlag": {
												"value": "@item().L2TransformationReqdFlag",
												"type": "Expression"
											},
											"DelayL1TransformationFlag": {
												"value": "@item().DelayL1TransformationFlag",
												"type": "Expression"
											},
											"DelayL2TransformationFlag": {
												"value": "@item().DelayL2TransformationFlag",
												"type": "Expression"
											},
											"DestinationRawFileSystem": {
												"value": "@item().DestinationRawFileSystem",
												"type": "Expression"
											},
											"DestinationRawFolder": {
												"value": "@item().DestinationRawFolder",
												"type": "Expression"
											},
											"DestinationRawFile": {
												"value": "@item().DestinationRawFile",
												"type": "Expression"
											},
											"SourceSQL": {
												"value": "@item().SourceSQL",
												"type": "Expression"
											},
											"StatSQL": {
												"value": "@item().StatSQL",
												"type": "Expression"
											},
											"ReloadFlag": {
												"value": "@item().ReloadFlag",
												"type": "Expression"
											},
											"ADFPipelineRunID": {
												"value": "@item().ADFPipelineRunID",
												"type": "Expression"
											},
											"BaseURL": {
												"value": "@pipeline().parameters.BaseURL",
												"type": "Expression"
											},
											"ServicePrincipalID": {
												"value": "@pipeline().parameters.ServicePrincipalID",
												"type": "Expression"
											},
											"TenantID": {
												"value": "@pipeline().parameters.TenantID",
												"type": "Expression"
											},
											"AADResource": {
												"value": "@pipeline().parameters.AADResource",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SourceSystemName": {
						"type": "string",
						"defaultValue": "Purview"
					},
					"StreamName": {
						"type": "string",
						"defaultValue": "%"
					},
					"MaxIngestInstance": {
						"type": "string",
						"defaultValue": "20"
					},
					"BaseURL": {
						"type": "string",
						"defaultValue": "https://ba-purview02-aug-pubpreview.purview.azure.com"
					},
					"ServicePrincipalID": {
						"type": "string",
						"defaultValue": "5e07b142-92b4-4671-83c0-e824bc93da6c"
					},
					"TenantID": {
						"type": "string",
						"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
					},
					"AADResource": {
						"type": "string",
						"defaultValue": "https://purview.azure.net"
					}
				},
				"folder": {
					"name": "REST API Pipelines"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/ControlDB_Dataset')]",
				"[concat(variables('workspaceId'), '/pipelines/IngestREST')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPool-Pause-Resume-Scale')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Call SQL Pool Logic App",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Set Logic App Parameters",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.logicapp_endpoint",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"value": "@json(variables('parameters'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set Logic App Parameters",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "parameters",
							"value": {
								"value": "{\n\"action\": \"@{pipeline().parameters.action}\",\n\"sku\": \"@{pipeline().parameters.sku}\",\n\"subscriptionId\": \"@{pipeline().parameters.subscriptionId}\",\n\"resourceGroupName\": \"@{pipeline().parameters.resourceGroupName}\",\n\"workspaceName\": \"@{pipeline().parameters.workspaceName}\",\n\"sqlPoolName\": \"@{pipeline().parameters.sqlPoolName}\",\n\"apiVersion\": \"@{pipeline().parameters.apiVersion}\"\n}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"logicapp_endpoint": {
						"type": "string"
					},
					"subscriptionId": {
						"type": "string"
					},
					"resourceGroupName": {
						"type": "string"
					},
					"workspaceName": {
						"type": "string"
					},
					"sqlPoolName": {
						"type": "string"
					},
					"apiVersion": {
						"type": "string",
						"defaultValue": "2021-03-01"
					},
					"action": {
						"type": "string",
						"defaultValue": "pause"
					},
					"sku": {
						"type": "string",
						"defaultValue": "DW100c"
					}
				},
				"variables": {
					"parameters": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Manage-SQLPool"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-07T14:22:37Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AtlasREST_API')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AtlasREST",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"RelativeUrl": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Azure/REST"
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@dataset().RelativeUrl",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AtlasREST')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_JSON')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Bronze",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileSystem": {
						"type": "string"
					},
					"Folder": {
						"type": "string"
					},
					"File": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Azure/Datalake"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().File",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().Folder",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().FileSystem",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Bronze')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze_Parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Bronze",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileSystem": {
						"type": "string"
					},
					"Folder": {
						"type": "string"
					},
					"File": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Azure/Datalake"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().File",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().Folder",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().FileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Bronze')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ControlDB_Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ControlDB",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Table": {
						"type": "string"
					},
					"Schema": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Azure/Datalake"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().Schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().Table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ControlDB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/REST_API')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "REST",
					"type": "LinkedServiceReference",
					"parameters": {
						"BaseURL": {
							"value": "@dataset().BaseURL",
							"type": "Expression"
						},
						"ServicePrincipalID": {
							"value": "@dataset().ServicePrincipalID",
							"type": "Expression"
						},
						"TenantID": {
							"value": "@dataset().TenantID",
							"type": "Expression"
						},
						"AADResource": {
							"value": "@dataset().AADResource",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"BaseURL": {
						"type": "string"
					},
					"ServicePrincipalID": {
						"type": "string"
					},
					"TenantID": {
						"type": "string"
					},
					"AADResource": {
						"type": "string"
					},
					"RelativeURL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Azure/REST"
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@dataset().RelativeURL",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/REST')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AtlasREST')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Endpoint for Purview Atlas REST APIs",
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('AtlasREST_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "AadServicePrincipal",
					"servicePrincipalId": "[parameters('AtlasREST_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyvault01",
							"type": "LinkedServiceReference"
						},
						"secretName": "ba-purview-spn"
					},
					"tenant": "[parameters('AtlasREST_properties_typeProperties_tenant')]",
					"aadResourceId": "[parameters('AtlasREST_properties_typeProperties_aadResourceId')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyvault01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bronze')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Bronze Zone of Datalake Storage",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Bronze_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ControlDB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Linked service for ELT Framework database",
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('ControlDB_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/REST')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Parametrized Linked Service for a REST API data source using Service Principal AAD Authentication",
				"parameters": {
					"BaseURL": {
						"type": "String"
					},
					"ServicePrincipalID": {
						"type": "String"
					},
					"TenantID": {
						"type": "String"
					},
					"AADResource": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('REST_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "AadServicePrincipal",
					"servicePrincipalId": "[parameters('REST_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyvault01",
							"type": "LinkedServiceReference"
						},
						"secretName": "ba-purview-spn"
					},
					"tenant": "[parameters('REST_properties_typeProperties_tenant')]",
					"aadResourceId": "[parameters('REST_properties_typeProperties_aadResourceId')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyvault01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SampleDB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('SampleDB_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ba-synapseanalytics01-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ba-synapseanalytics01-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ba-synapseanalytics01-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ba-synapseanalytics01-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bamachinelearningws')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('bamachinelearningws_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('bamachinelearningws_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "ba-machinelearningws01",
					"servicePrincipalId": "[parameters('bamachinelearningws_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyvault01",
							"type": "LinkedServiceReference"
						},
						"secretName": "synapse-machineleaningws-spn"
					},
					"tenant": "[parameters('bamachinelearningws_properties_typeProperties_tenant')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyvault01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyvault01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyvault01_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/storagecosmosdb1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('storagecosmosdb1_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Azure Daily')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Daily Trigger to Extract, Load and Transform data from Azure REST APIs",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Master ELT REST API",
							"type": "PipelineReference"
						},
						"parameters": {
							"SourceSystemName": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_SourceSystemName')]",
							"StreamName": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_StreamName')]",
							"MaxIngestInstance": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_MaxIngestInstance')]",
							"BaseURL": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_BaseURL')]",
							"ServicePrincipalID": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_ServicePrincipalID')]",
							"TenantID": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_TenantID')]",
							"AADResource": "[parameters('Azure Daily_properties_Master ELT REST API_parameters_AADResource')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2022-07-01T00:49:00",
						"timeZone": "AUS Eastern Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								4
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Master ELT REST API')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pause-ALL-Dedicated-SQLPools')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Manage-All-SQLPools",
							"type": "PipelineReference"
						},
						"parameters": {
							"resourceList_logicapp_endpoint": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_resourceList_logicapp_endpoint')]",
							"subscriptionId": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_subscriptionId')]",
							"resourceType": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_resourceType')]",
							"apiVersion": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_apiVersion')]",
							"action": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_action')]",
							"sku": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_sku')]",
							"sqlPool_logicapp_endpoint": "[parameters('Pause-ALL-Dedicated-SQLPools_properties_Manage-All-SQLPools_parameters_sqlPool_logicapp_endpoint')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2021-07-11T19:00:00",
						"timeZone": "AUS Eastern Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								19
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Manage-All-SQLPools')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Purview Daily')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Daily trigger to refresh Purview data",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Master ELT REST API",
							"type": "PipelineReference"
						},
						"parameters": {
							"SourceSystemName": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_SourceSystemName')]",
							"StreamName": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_StreamName')]",
							"MaxIngestInstance": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_MaxIngestInstance')]",
							"BaseURL": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_BaseURL')]",
							"ServicePrincipalID": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_ServicePrincipalID')]",
							"TenantID": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_TenantID')]",
							"AADResource": "[parameters('Purview Daily_properties_Master ELT REST API_parameters_AADResource')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2022-06-30T00:00:00",
						"timeZone": "AUS Eastern Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								5
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Master ELT REST API')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EnrichData_With_AML Model')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROCEDURE dbo.nyc_taxi_procedure\nAS\nBEGIN\n\nSELECT\n    CAST([paymentType] AS [bigint]) AS [paymentType],\n    CAST([passengerCount] AS [bigint]) AS [passengerCount],\n    [tripTimeSecs],\n    CAST([pickupTimeBin] AS [varchar]) AS [pickupTimeBin]\nINTO [dbo].[#nyc_taxi]\nFROM [dbo].[nyc_taxi];\n\nSELECT *\nFROM PREDICT (MODEL = (SELECT [model] FROM dbo.nyc_taxi_tip_aml_model WHERE [ID] = 'nyc_taxi_tip_predict:3'),\n              DATA = [dbo].[#nyc_taxi],\n              RUNTIME = ONNX) WITH ([output_label] [bigint])\n\nEND\nGO\n\nEXEC dbo.nyc_taxi_procedure",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dw01",
						"poolName": "dw01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless - Query Parquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n  top 100  *\nFROM\n    OPENROWSET(\n        BULK 'https://bastoragedatalake01.dfs.core.windows.net/raw-bronze/purview/operations/2022-07/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AAD-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Reusable functions for Azure Active Directory",
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "24ad8aed-5cb6-4987-a34a-7b394600fa7f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable functions for Azure Active Directory"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"keyvault01\"}"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"tenandIdSecret = \"<Azure Key Vault Secret for TenantID>\"\r\n",
							"servicePrincipalIdSecret = \"<Azure Key Vault Secret for Service Principal ID>\"\r\n",
							"servicePrincipalSecret = \"<Azure Key Vault Secret for Service Principal secret>\"\r\n",
							"authUrl = \"https://login.windows.net\"\r\n",
							"resourceUrl = \"https://database.windows.net/\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import adal\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# getBearerToken()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def getBearerToken():\r\n",
							"    ############################################################################\r\n",
							"    # getBearerToken\r\n",
							"    # Returns a bearer token for a service principal AAD authentication\r\n",
							"    #\r\n",
							"    # Parameters:\r\n",
							"    #       None\r\n",
							"    #\r\n",
							"    # Returns:\r\n",
							"    #     Bearer Token  \r\n",
							"    ############################################################################\r\n",
							"\r\n",
							"    tenantId = getSecret(tenandIdSecret)\r\n",
							"    servicePrincipalId = getSecret(servicePrincipalIdSecret)\r\n",
							"    secret = getSecret(servicePrincipalSecret)\r\n",
							"\r\n",
							"    assert tenantId is not None, \"tenantId not specified\"\r\n",
							"    assert servicePrincipalId is not None, \"servicePrincipalId not specified\"\r\n",
							"    assert secret is not None, \"secret not specified\"\r\n",
							"    assert authUrl is not None, \"authUrl not specified\"\r\n",
							"    assert resourceUrl is not None, \"resourceUrl not specified\"\r\n",
							"\r\n",
							"    authority = authUrl + \"/\" + tenantId\r\n",
							"    try:\r\n",
							"        context = adal.AuthenticationContext(authority)\r\n",
							"        token = context.acquire_token_with_client_credentials(resourceUrl, servicePrincipalId, secret)\r\n",
							"        accessToken = token[\"accessToken\"]\r\n",
							"    except Exception as e:\r\n",
							"        print(\"getBearerToken failed with exception:\")\r\n",
							"        raise e\r\n",
							"    return accessToken"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Reusable functions for Azure SQL and SQL Server",
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "d1563366-0f0f-49b2-afc7-6ae03ec11eba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable functions for Azure SQL and SQL Server"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"keyvault01\"}"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"server=\"<Azure SQL Server/ SQL Server>\"\r\n",
							"database = \"<Database Name>\"\r\n",
							"usernameSecret = \"<Azure Key Vault Secret for SQL User>\"\r\n",
							"passwordSecret = \"<Azure Key Vault Secret for SQL User Password>\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# getJdbcUrl()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def getJdbcUrl():\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: getJdbcUrl\r\n",
							"  # Creates the JDBC Connection String from the input parameters to this notebook\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # None\r\n",
							"  #\r\n",
							"  # Returns:\r\n",
							"  # JDBC Connection String\r\n",
							"  # ##########################################################################################################################  \r\n",
							"    assert server is not None, \"server not specified\"\r\n",
							"    assert database is not None, \"database not specified\"\r\n",
							"    assert usernameSecret is not None, \"usernameSecret not specified\"\r\n",
							"    assert passwordSecret is not None, \"passwordSecret not specified\"\r\n",
							"\r\n",
							"    user =  getSecret(usernameSecret)\r\n",
							"    password = getSecret(passwordSecret)\r\n",
							"    jdbcUrl = \"jdbc:sqlserver://\" + server + \".database.windows.net:1433;database=\" + database + \";user=\" + user + \"@\" + server + \";password=\" + password + \";encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\r\n",
							"    return jdbcUrl"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# readTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def readTable(table):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: readTable\r\n",
							"  # Reads all the rows and columns of an Azure SQL table/view and returns the records as dataframe\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # table = Input Table/View including the schema name. E.g soccer.goalpost, afl.goalpost\r\n",
							"  #\r\n",
							"  # Returns:\r\n",
							"  # Dataframe containing all rows and columns of a table/view in Azure SQL\r\n",
							"  # ##########################################################################################################################  \r\n",
							"    assert table is not None, \"table not specified\"\r\n",
							"\r\n",
							"    jdbcUrl = getJdbcUrl()\r\n",
							"\r\n",
							"    try:\r\n",
							"      df = (spark.read\r\n",
							"              .format(\"jdbc\")\r\n",
							"              .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
							"              .option(\"url\",jdbcUrl)\r\n",
							"              .option(\"dbtable\",table)\r\n",
							"              .load()\r\n",
							"          )\r\n",
							"    except Exception as e:\r\n",
							"        print(\"readTable({}) failed with exception:\".format(table))\r\n",
							"        raise e\r\n",
							"\r\n",
							"\r\n",
							"    ## Use Apache Spark Connector in Future instead of JDBC connector for faster execution. At the moment Apache Spark Connector is not available on newer Spark Runtime\r\n",
							"    ## Have to install a library from maven coordinates:com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\r\n",
							"    ## https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver16\r\n",
							"    #   df = (spark.read\r\n",
							"    #           .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"    #           .option(\"url\",jdbcUrl)\r\n",
							"    #           .option(\"dbtable\",table)\r\n",
							"    #           .load()\r\n",
							"    #       )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# readQuery()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def readQuery(query):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: readQuery\r\n",
							"  # Executes the input query at Azure SQL and returns the records as dataframe\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # query = A valid input query\r\n",
							"  #\r\n",
							"  # Returns:\r\n",
							"  # Dataframe containing all rows and columns returned by query from Azure SQL\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  assert query is not None, \"query not specified\"\r\n",
							"\r\n",
							"  jdbcUrl = getJdbcUrl()\r\n",
							"  try:\r\n",
							"    df = (spark.read\r\n",
							"            .format(\"jdbc\")\r\n",
							"            .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
							"            .option(\"url\",jdbcUrl)\r\n",
							"            .option(\"query\",query)\r\n",
							"            .load()\r\n",
							"        )\r\n",
							"  except Exception as e:\r\n",
							"        print(\"readQuery({}) failed with exception:\".format(query))\r\n",
							"        raise e\r\n",
							"    ## Use Apache Spark Connector in Future instead of JDBC connector for faster execution. At the moment Apache Spark Connector is not available on newer Spark Runtime\r\n",
							"    ## Have to install a library from maven coordinates:com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\r\n",
							"    ## https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver16\r\n",
							"    #   df = (spark.read\r\n",
							"    #           .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"    #           .option(\"url\",jdbcUrl)\r\n",
							"    #           .option(\"dbtable\",table)\r\n",
							"    #           .load()\r\n",
							"    #       )\r\n",
							"  return df"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# insertTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def insertTable(df,table,writeMode,preserveSchema = None):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: insertTable\r\n",
							"  # Inserts a dataframe to a Azure SQL\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # df = Input dataframe\r\n",
							"  # table = Target table where the datafframe is loaded\r\n",
							"  # writeMode = Describes how data from dataframe is inserted to datawarehouse table. Allowed values are - append/overwrite/ignore/error/errorifexists\r\n",
							"  # preserveSchema = True/False. Applicable only in overwrite mode when schema needs to be preserved\r\n",
							"  # Returns:\r\n",
							"  # None\r\n",
							"  # ########################################################################################################################## \r\n",
							"  assert table is not None, \"query not specified\"\r\n",
							"  assert writeMode is not None, \"writeMode not specified\"\r\n",
							"  assert writeMode in [\"append\",\"overwrite\",\"ignore\",\"error\",\"errorifexists\"] , \"writeMode is invalid. Allowed values are append,overwrite,ignore,error,errorifexists\"\r\n",
							"\r\n",
							"  jdbcUrl = getJdbcUrl()\r\n",
							"  \r\n",
							"  try:\r\n",
							"    if writeMode == \"overwrite\" and preserveSchema == True :\r\n",
							"      #Truncate and Append instead of overwrite to preserve table schema\r\n",
							"      (df.write\r\n",
							"      .format(\"jdbc\")\r\n",
							"      .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
							"      .option(\"url\",jdbcUrl)\r\n",
							"      .mode(\"append\")\r\n",
							"      .option(\"truncate\", \"true\") # Must be string \"true\" and not boolean True\r\n",
							"      .option(\"dbtable\",table)\r\n",
							"      .save()\r\n",
							"      )\r\n",
							"    else:\r\n",
							"      (df.write\r\n",
							"      .format(\"jdbc\")\r\n",
							"      .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
							"      .option(\"url\",jdbcUrl)\r\n",
							"      .mode(writeMode)\r\n",
							"      .option(\"dbtable\",table)\r\n",
							"      .save()\r\n",
							"      ) \r\n",
							"  except Exception as e:\r\n",
							"        print(\"insertTable({}) failed with exception:\".format(query))\r\n",
							"        raise e \r\n",
							"    ## Use Apache Spark Connector in Future instead of JDBC connector for faster execution. At the moment Apache Spark Connector is not available on newer Spark Runtime\r\n",
							"    ## Have to install a library from maven coordinates:com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\r\n",
							"    ## https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver16\r\n",
							"    #   df = (spark.read\r\n",
							"    #           .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"    #           .option(\"url\",jdbcUrl)\r\n",
							"    #           .option(\"dbtable\",table)\r\n",
							"    #           .load()\r\n",
							"    #       )"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# upsert()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upsert(df,SchemaStagingTable,StagingTable, SchemaTargetTable, TargetTable, KeyColumns,IdentityColumns=None, DeltaColumn=None) :\r\n",
							"# ##########################################################################################################################  \r\n",
							"# Function: upsert\r\n",
							"# Performs a Merge/Upsert action on a Azure SQL table\r\n",
							"# \r\n",
							"# Parameters:\r\n",
							"# df = Input dataframe\r\n",
							"# StagingTable = Name of Table used to temporarily stage the input data frame\r\n",
							"# SchemaStagingTable = Database schema of staging table\r\n",
							"# SchemaTargetTable = Database schema of target table\r\n",
							"# TargetTable  = Name of Target Table\r\n",
							"# KeyColumns = List of columns that uniquely defines a record in input dataframe\r\n",
							"# IdentityColumns = List of Identity Columns that needs to be excluded from Inserts and Updates\r\n",
							"# DeltaColumn = Name of watermark column in input dataframe\r\n",
							"#\r\n",
							"# Returns:\r\n",
							"# None\r\n",
							"# ##########################################################################################################################  \r\n",
							"\r\n",
							"    insertTable(df,SchemaStagingTable + \".\" + StagingTable,\"overwrite\")\r\n",
							"    \r\n",
							"\r\n",
							"    insertCols=\"\"\r\n",
							"    updateCols=\"\"\r\n",
							"    whereCols =\"\"\r\n",
							"    keyCols = \"\"\r\n",
							"\r\n",
							"    # MATCHED Clause of MERGE\r\n",
							"    for keyCol in KeyColumns:\r\n",
							"        keyCols = keyCols + \"source.\" + keyCol + \" = \" + \"target.\" + keyCol + \" and \"\r\n",
							"\r\n",
							"    whereClause = keyCols\r\n",
							"    #Tidy up where clause and remove last \"and\"\r\n",
							"    remove=\"and\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    whereClause = whereClause[::-1].replace(reverse_remove,\"\",1)[::-1]\r\n",
							"\r\n",
							"    # INSERT and UPDATE part of MERGE\r\n",
							"    for col in df.schema.fieldNames():\r\n",
							"        if col not in IdentityColumns: # Exclude Identity columns in Insert statement\r\n",
							"            insertCols = insertCols + col + \",\"\r\n",
							"        if col not in KeyColumns and col not in IdentityColumns: # Exclude key and Identity columns in update statement\r\n",
							"            updateCols =  updateCols + \"target.\" + col + \" = \" + \"source.\" + col + \",\"\r\n",
							"\r\n",
							"    #Tidy up insertCols and remove last \",\"\r\n",
							"    remove=\",\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    insertCols = insertCols[::-1].replace(reverse_remove,\"\",1)[::-1]  \r\n",
							"\r\n",
							"    #Tidy up insertCols and remove last \",\"\r\n",
							"    remove=\",\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    updateCols = updateCols[::-1].replace(reverse_remove,\"\",1)[::-1]   \r\n",
							"\r\n",
							"    # MERGE Statement in full\r\n",
							"    mergeSQL= \"MERGE INTO \" + SchemaTargetTable +\".\" + TargetTable + \" AS target\" +  \" USING \" + SchemaStagingTable + \".\" + StagingTable + \" AS source\" + \" ON \"\r\n",
							"    mergeSQL = mergeSQL + whereClause\r\n",
							"\r\n",
							"    # WHEN MATCHED\r\n",
							"    if DeltaColumn != None:\r\n",
							"        mergeSQL =  mergeSQL + \" WHEN MATCHED AND \" + \"source.\" + DeltaColumn + \" > \" + \"target.\" + DeltaColumn \r\n",
							"    else:\r\n",
							"        mergeSQL =  mergeSQL + \" WHEN MATCHED\"\r\n",
							"\r\n",
							"    mergeSQL = mergeSQL + \" THEN UPDATE SET \" + updateCols\r\n",
							"\r\n",
							"    # WHEN NOT MATCHED BY TARGET\r\n",
							"    mergeSQL = mergeSQL + \" WHEN NOT MATCHED BY TARGET \" # including BY TARGET clause for Synapse requires the table to be Hash Distributed\r\n",
							"    mergeSQL = mergeSQL + \" THEN INSERT \" +  \"(\"+ insertCols +\")\" + \" VALUES \" + \" ( \"+ insertCols +\" ) \"\r\n",
							"\r\n",
							"    mergeSQL = mergeSQL +\";\"\r\n",
							"    \r\n",
							"    #Execute Merge Statement using Pyodbc\r\n",
							"    uid = getSecret(usernameSecret)\r\n",
							"    pwd = getSecret(passwordSecret)\r\n",
							" \r\n",
							"    try:\r\n",
							"        cnxn = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + server + \".database.windows.net\" + \";DATABASE=\" + database + \";UID=\" + uid + \";PWD=\" + pwd )\r\n",
							"        cursor = cnxn.cursor()\r\n",
							"        cnxn.autocommit = True\r\n",
							"\r\n",
							"        cursor.execute(mergeSQL)\r\n",
							"    except Exception as e:\r\n",
							"        print(\"Upsert to {}.{} failed with exception:\".format(SchemaTargetTable,TargetTable))\r\n",
							"        raise e\r\n",
							"\r\n",
							"    print(\"Upsert statement executed successfully : {} \".format(mergeSQL))\r\n",
							"    return"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/commonTransforms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "A collection of reusable Python classes that extends out of box PySpark capabilities",
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "fdaa8188-85d6-44bb-b31c-f2f5148009c9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# What is CommonTransforms and how to use them in your notebooks ?  \r\n",
							"CommonTransforms is a Python class that uses PySpark libraries to apply common transformations to a Spark dataframe. https://github.com/bennyaustin/pyspark-utils/blob/main/CommonTransforms/README.md"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# CommonTransforms Class"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import trim,when,isnull,lit,col,from_utc_timestamp,to_utc_timestamp,concat_ws,sha1,length,substring,lit,concat,date_add,expr,year,datediff\r\n",
							"from pyspark.sql import functions as F \r\n",
							"import datetime"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class CommonTransforms:\r\n",
							"  inputDf=None\r\n",
							"  inputSchema=None\r\n",
							"  inputColums=None\r\n",
							"  \r\n",
							"#   Constructor\r\n",
							"  def __init__(self, input):\r\n",
							"    self.inputDf=input\r\n",
							"    self.inputSchema=self.inputDf.schema\r\n",
							"    self.inputColumns=self.inputDf.schema.fieldNames()\r\n",
							"    \r\n",
							"#  Remove Leading and Trailing Spaces \r\n",
							"  def trim(self):\r\n",
							"    stringCol= (col for col in self.inputSchema if str(col.dataType)==\"StringType\")\r\n",
							"    for col in stringCol:\r\n",
							"        self.inputDf = self.inputDf.withColumn(col.name,trim(col.name))\r\n",
							"    return self.inputDf\r\n",
							"  \r\n",
							"#   Replace Null values with Default values based on datatypes\r\n",
							"  def replaceNull(self,value, subset=None):\r\n",
							"    isDate=False\r\n",
							"    isTimestamp =False\r\n",
							"    \r\n",
							"    try:\r\n",
							"      if isinstance(value, str):\r\n",
							"        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%d\") #YYYY-MM-DD format e.g \"2020-10-01\"\r\n",
							"        isDate= True\r\n",
							"    except ValueError:\r\n",
							"      isDate=False\r\n",
							"      \r\n",
							"    try:\r\n",
							"      if isinstance(value, str):\r\n",
							"        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\") #YYYY-MM-DDThh:mm:ss format e.g \"2020-10-01T19:50:06\"\r\n",
							"        isTimestamp= True\r\n",
							"    except ValueError:\r\n",
							"      isTimestamp=False\r\n",
							"      \r\n",
							"    if isDate and subset is not None:\r\n",
							"      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True and x.name in subset)\r\n",
							"      for x in dateCol:\r\n",
							"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
							"    elif isDate and subset is None:\r\n",
							"      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True)\r\n",
							"      for x in dateCol:\r\n",
							"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
							"    elif isTimestamp and subset is not None:\r\n",
							"      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True and x.name in subset)\r\n",
							"      for x in tsCol:\r\n",
							"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\r\n",
							"    elif isTimestamp and subset is None:\r\n",
							"      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True)\r\n",
							"      for x in tsCol:\r\n",
							"        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))        \r\n",
							"    else:\r\n",
							"      self.inputDf = self.inputDf.fillna(value,subset)\r\n",
							"      \r\n",
							"    return self.inputDf\r\n",
							"\r\n",
							"#  Remove duplicates\r\n",
							"  def deDuplicate(self, subset=None):\r\n",
							"    self.inputDf = self.inputDf.dropDuplicates(subset)\r\n",
							"    return self.inputDf\r\n",
							"  \r\n",
							"#   Convert UTC timestamp to local\r\n",
							"  def utc_to_local(self,localTimeZone,subset=None):\r\n",
							"    if subset is not None:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
							"    else:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
							"      \r\n",
							"    for x in tsCol:\r\n",
							"      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),localTimeZone))\r\n",
							"    return self.inputDf\r\n",
							"\r\n",
							"#   Convert timestamp in local timezone to UTC\r\n",
							"  def local_to_utc(self,localTimeZone,subset=None):\r\n",
							"    if subset is not None:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
							"    else:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
							"      \r\n",
							"    for x in tsCol:\r\n",
							"      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),localTimeZone))\r\n",
							"    return self.inputDf\r\n",
							"  \r\n",
							"#   Change Timezone\r\n",
							"  def changeTimezone(self,fromTimezone,toTimezone,subset=None):\r\n",
							"    if subset is not None:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\r\n",
							"    else:\r\n",
							"      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\r\n",
							"    \r\n",
							"    for x in tsCol:\r\n",
							"      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),fromTimezone))\r\n",
							"      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),toTimezone))\r\n",
							"    return self.inputDf\r\n",
							"\r\n",
							"#   Drop System/Non-Business Columns\r\n",
							"  def dropSysColumns(self,columns):\r\n",
							"    self.inputDf = self.inputDf.drop(columns)\r\n",
							"    return self.inputDf \r\n",
							"  \r\n",
							"#  Create Checksum Column \r\n",
							"  def addChecksumCol(self,colName):\r\n",
							"    self.inputDf = self.inputDf.withColumn(colName,sha1(concat_ws(\"~~\", *self.inputDf.columns)))\r\n",
							"    return self.inputDf\r\n",
							"\r\n",
							"# Convert Julian Date to Calendar Date  \r\n",
							"  def julian_to_calendar(self,subset):\r\n",
							"    julCol = (x for x in self.inputSchema if str(x.dataType)==\"IntegerType\" and x.name in subset)\r\n",
							"    for x in julCol:\r\n",
							"      self.inputDf = (self.inputDf.withColumn(x.name,col(x.name).cast(\"string\"))\r\n",
							"                                 .withColumn(x.name+\"_year\",\r\n",
							"                                             when((length(col(x.name))==5) & (substring(col(x.name),1,2) <=50),concat(lit('20'),substring(col(x.name),1,2)))\r\n",
							"                                             .when((length(col(x.name))==5) & (substring(col(x.name),1,2) >50),concat(lit('19'),substring(col(x.name),1,2)))\r\n",
							"                                             .when(length(col(x.name))==7,substring(col(x.name),1,4))\r\n",
							"                                             .otherwise(lit(0))\r\n",
							"                                            )\r\n",
							"                                 .withColumn(x.name+\"_days\",\r\n",
							"                                             when(length(col(x.name))==5,substring(col(x.name),3,3).cast(\"int\"))\r\n",
							"                                             .when(length(col(x.name))==7,substring(col(x.name),5,3).cast(\"int\"))\r\n",
							"                                             .otherwise(lit(0))\r\n",
							"                                            )\r\n",
							"                                 .withColumn(x.name+\"_ref_year\",concat(col(x.name+\"_year\"),lit(\"-01\"),lit(\"-01\")).cast(\"date\"))\r\n",
							"                                 .withColumn(x.name+\"_calendar\",expr(\"date_add(\" + x.name+\"_ref_year\"+\",\"+ x.name+\"_days)-1\"))\r\n",
							"                                 .drop(x.name, x.name+\"_year\",x.name+\"_days\",x.name+\"_ref_year\")\r\n",
							"                                 .withColumnRenamed(x.name+\"_calendar\",x.name)\r\n",
							"                                 \r\n",
							"                     )\r\n",
							"    return self.inputDf \r\n",
							"  \r\n",
							"# Convert Calendar Date to Julian Date \r\n",
							"  def calendar_to_julian(self, subset):\r\n",
							"    calCol = (x for x in self.inputSchema if ((str(x.dataType)==\"DateType\" or str(x.dataType)==\"TimestampType\") and x.name in subset))\r\n",
							"\r\n",
							"    for x in calCol:\r\n",
							"      self.inputDf = (self.inputDf.withColumn(x.name+\"_ref_year\", concat(year(col(x.name)).cast(\"string\"),lit(\"-01\"),lit(\"-01\")))\r\n",
							"                                  .withColumn(x.name+\"_datediff\", datediff(col(x.name),col(x.name+\"_ref_year\"))+1)\r\n",
							"                                  .withColumn(x.name+\"_julian\", concat(substring(year(col(x.name)).cast(\"string\"),3,2),col(x.name+\"_datediff\")).cast(\"int\"))\r\n",
							"                                  .drop(x.name,x.name+\"_ref_year\",x.name+\"_datediff\")\r\n",
							"                                  .withColumnRenamed(x.name+\"_julian\",x.name)\r\n",
							"                     )\r\n",
							"    return self.inputDf\r\n",
							"\r\n",
							"# Add a set of literal value columns to dataframe, pass as dictionary parameter  \r\n",
							"  def addLitCols(self,colDict):\r\n",
							"    for x in colDict.items():\r\n",
							"      self.inputDf = self.inputDf.withColumn(x[0],lit(x[1]))\r\n",
							"    return self.inputDf"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datalake-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "821b3d1a-f02b-4c1d-8a50-8b78c31f0f04"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable Datalake Functions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# readFile()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def readFile(storageAccount, container, path, colSeparator, headerFlag):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: readFile\r\n",
							"  # Reads a file from Azure Gen2 Storage and returns as dataframe\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # storageAccount = Name of Storage Account  \r\n",
							"  # container = File System/Container of Azure Data Lake Storage\r\n",
							"  # path = realtive path of file including folder name, file name and file extension. For e.g /folder/file.extension\r\n",
							"  # colSeparator = Column separator for text files\r\n",
							"  # headerFlag = boolean flag to indicate whether the text file has a header or not  \r\n",
							"  # \r\n",
							"  # Returns:\r\n",
							"  # A dataframe of the raw file\r\n",
							"  # ##########################################################################################################################    \r\n",
							"\r\n",
							"    filePath = \"abfss://\" + container + \"@\"+ storageAccount + \".dfs.core.windows.net/\" + path\r\n",
							"    if \".csv\" in path or \".txt\" in path:\r\n",
							"        df = spark.read.csv(path=filePath, sep=colSeparator, header=headerFlag, inferSchema=\"true\")\r\n",
							"    elif \".parquet\" in path:\r\n",
							"        df = spark.read.parquet(filePath)\r\n",
							"    elif \".orc\" in path:\r\n",
							"        df = spark.read.orc(filePath)\r\n",
							"    else:\r\n",
							"        df = spark.read.format(\"csv\").load(filePath)\r\n",
							"  \r\n",
							"    df =df.dropDuplicates()\r\n",
							"    return df\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# writeFile()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def writeFile(df,storageAccount, container, path, writeMode=\"overwrite\", colSeparator=\",\"):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: writeFile\r\n",
							"  # Writes the input dataframe to a file in Azure Gen2 Storage\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  # df= input dataframe\r\n",
							"  # storageAccount = Name of Storage Account\r\n",
							"  # container = File System/Container of Azure Data Lake Storage\r\n",
							"  # path = realtive path of file including folder name, file name and file extension. For e.g /folder/file.extension\r\n",
							"  # writeMode= mode of writing the curated file. Allowed values - append/overwrite/ignore/error/errorifexists\r\n",
							"  # colSeparator = Column separator for text files\r\n",
							"  # \r\n",
							"  # Returns:\r\n",
							"  # A dataframe of the raw file\r\n",
							"  # ##########################################################################################################################     \r\n",
							"    filePath = \"abfss://\" + container + \"@\"+ storageAccount + \".dfs.core.windows.net/\" + path\r\n",
							"    if \"csv\" in path or 'txt' in path:\r\n",
							"        df.write.csv(filePath,mode=writeMode,sep=colSeparator,header=\"true\")\r\n",
							"    elif \"parquet\" in path:\r\n",
							"        df.write.parquet(filePath,mode=writeMode)\r\n",
							"    elif \"orc\" in path:\r\n",
							"        df.write.orc(filePath,mode=writeMode)\r\n",
							"    elif \"json\" in path:\r\n",
							"        df.write.json(filePath, mode=writeMode)\r\n",
							"    else:\r\n",
							"        df.write.save(path=filePath,format=\"csv\",mode=writeMode)\r\n",
							"    return\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dedicatedSQLPool-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "2e9e7d37-4610-4027-970f-cae3af6999f5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable functions for Azure Synapse SQL Dedicated Pool"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"keyvault01\"}"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"server=\"<SynapseWorkspaceName>.sql.azuresynapse.net,1433\"\r\n",
							"database = \"<DedicatedSQLPool>\""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"import sys"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## readSdpTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def readSdpTable(schema,table,colList=None,filterCondition=None,limitRows=None):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: readSdpTable\r\n",
							"  # Reads records from the Azure Synapse Dedicated Pool table/view and returns as dataframe\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  #     schema          = Schema name of the Azure Synapse Dedicated Pool table.\r\n",
							"  #     table           = Table Name.\r\n",
							"  #     colList         = (Optional) List of columns to be returned in the dataframe. \r\n",
							"  #                       E.g [\"col1\",\"col2\",\"col3\"].\r\n",
							"  #                       Returns all columns if none provided.\r\n",
							"  #     filterCondition = (Optional) Expression to filter the dataframe as push-down filter to database. \r\n",
							"  #                       E.g col(\"Title\").contains(\"E\").\r\n",
							"  #                       Returns all rows when not provided.\r\n",
							"  #     limitRows       = (Optional) Integer to fetch N records from the table.\r\n",
							"  #                       Returns all rows when none provided.\r\n",
							"  #\r\n",
							"  # Returns:\r\n",
							"  #     Dataframe containing the relevant rows and columns of an Azure Synapse Dedicated Pool table/view\r\n",
							"  # ##########################################################################################################################  \r\n",
							"    assert server is not None, \"Server not specified\"\r\n",
							"    assert database is not None, \"Database not specified\"\r\n",
							"    assert schema is not None, \"Schema not specified\"\r\n",
							"    assert table is not None, \"Table not specified\"\r\n",
							"\r\n",
							"    tableName = database + \".\" + schema+ \".\" + table\r\n",
							"   \r\n",
							"    try:\r\n",
							"        if server is not None and database is not None and schema is not None and table is not None:\r\n",
							"            df = (spark.read\r\n",
							"                        .option(Constants.SERVER,server)\r\n",
							"                        .synapsesql(tableName)\r\n",
							"                )\r\n",
							"\r\n",
							"        if colList is not None:\r\n",
							"            df = df.select(colList)\r\n",
							"        \r\n",
							"        if filterCondition is not None:\r\n",
							"            df = df.filter(filterCondition)\r\n",
							"\r\n",
							"        if limitRows is not None:\r\n",
							"            df = df.limit(limitRows)\r\n",
							"    except Exception as e:\r\n",
							"        print(\"Read from {}.{} failed with exception:\".format(schema,table))   \r\n",
							"        raise e\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## insertSdpTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def insertSdpTable(df, schema,table, mode):\r\n",
							"  # ##########################################################################################################################  \r\n",
							"  # Function: insertSdpTable\r\n",
							"  # Writes a dataframe to an Azure Synapse Dedicated Pool table\r\n",
							"  # \r\n",
							"  # Parameters:\r\n",
							"  #     df              = Dataframe to be written to Azure Synapse Dedicated Pool table.\r\n",
							"  #     schema          = Schema name of the Azure Synapse Dedicated Pool table.\r\n",
							"  #     table           = Table Name.\r\n",
							"  #     mode            = Write mode to table. \r\n",
							"  #                       Options for write modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							"  #\r\n",
							"  # Returns:\r\n",
							"  #     None\r\n",
							"  # ##########################################################################################################################  \r\n",
							"    assert server is not None, \"Server not specified\"\r\n",
							"    assert database is not None, \"Database not specified\"\r\n",
							"    assert schema is not None, \"Schema not specified\"\r\n",
							"    assert table is not None, \"Table not specified\"\r\n",
							"    assert mode in [\"append\",\"overwrite\",\"error\",\"errorifexists\",\"ignore\"], \"Invalid mode specified. Mode should be either append,overwrite,error,errorifexists or ignore\"\r\n",
							"\r\n",
							"    tableName = database + \".\" + schema+ \".\" + table\r\n",
							"    try:\r\n",
							"      (df.write\r\n",
							"          .option(Constants.SERVER, server)\r\n",
							"          .mode(mode)\r\n",
							"          .synapsesql(tableName))\r\n",
							"    except Exception as e:\r\n",
							"      print(\"Insert to {}.{} failed with exception:\".format(schema,table))\r\n",
							"      raise e\r\n",
							"    return\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyvault-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "83d3d463-7bc1-4b84-a091-cb1ff4c7c0e8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable Azure Key Vault Functions"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Parameters and Init"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"kvLinkedService ='your Azure Key Vault Linked Service referenced by this Synapse Workspace'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## getSecret()\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def getSecret (secretName):\n",
							"# ##########################################################################################################################  \n",
							"# Function: getSecret\n",
							"# Returns the value of a secret stored in Azure Key Vault that is referenced as link service\n",
							"# \n",
							"# Parameters:\n",
							"# secretName = Name of secret in Azure Key Vault\n",
							"# \n",
							"# Returns:\n",
							"# The value of secret stored in Azure Key Vault\n",
							"# ##########################################################################################################################\n",
							"    assert secretName is not None, \"secretName not specified\"\n",
							"    try:\n",
							"        return mssparkutils.credentials.getSecretWithLS(kvLinkedService,secretName) #kvLinkedService is a notebook parameter set by the calling notebook\n",
							"    except Exception as e:\n",
							"        print(\"Either Linked Service: \" + kvLinkedService + \" does not exist or Secret: \" + secretName + \" does not exist\")\n",
							"        raise e"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-AAD-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook to unit test AAD-functions",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "9ab9a03e-2d18-44cb-b36b-a4efb402ff10"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test AAD-functions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run /common/AAD-functions {\"tenandIdSecret\":\"TenantID\", \"servicePrincipalIdSecret\": \"ba-automation-spn-clientID\",\"servicePrincipalSecret\": \"ba-automation-spn\",\"authUrl\": \"https://login.windows.net\",\"resourceUrl\": \"https://database.windows.net/\"}"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test getBearerToken()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bearerToken =getBearerToken()\r\n",
							"print(bearerToken)"
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-SQL-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook to unit test SQL-functions",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "e411df1a-c212-47a0-bb35-e4fcaa789d1b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test SQL-functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"%run /common/SQL-functions {\"server\":\"ba-sqlserver1\", \"database\": \"AdventureWorks\",\"usernameSecret\": \"ba-sqlserver1-sqlusername\", \"passwordSecret\": \"ba-sqlserver1-password\"}"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test readTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = readTable (\"Person.Address\")\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test readQuery()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"query =(\"select Person.BusinessEntityAddress.AddressID, Person.AddressType.NAME\"\r\n",
							"        + \" from Person.BusinessEntityAddress \"\r\n",
							"        + \" inner join Person.AddressType \"\r\n",
							"       + \" ON Person.AddressType.AddressTypeID = Person.BusinessEntityAddress.AddressTypeID\")\r\n",
							"\r\n",
							"df = readQuery(query)\r\n",
							"display(df)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test insertTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Overwrite, Create new table if doesn't exist\r\n",
							"insertTable(df,\"dbo.test\",\"overwrite\",preserveSchema = False)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Overwrite, Preserve schema if table exists\r\n",
							"insertTable(df,\"dbo.test\",\"overwrite\",preserveSchema = True)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Append\r\n",
							"insertTable(df,\"dbo.test\",\"append\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Test Upsert()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = readTable (\"Person.Address\")\r\n",
							"\r\n",
							"upsert(df,\"dbo\",\"AddressStage\", \"dbo\", \"AddressMerge\", [\"AddressID\"], [\"AddressID\"],\"ModifiedDate\") "
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-commonTransforms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook to unit test commonTransforms class",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "d49ef055-9c02-4bea-8e60-527637a12d82"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import DataFrame,Column,functions,types"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/commonTransforms"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/datalake-functions"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Test File\r\n",
							"file = \"abfss://raw-bronze@bastoragedatalake01.dfs.core.windows.net/nyc-yellowtaxi-trip/yellow_tripdata_2020-01.csv\"\r\n",
							"# Config schema explicitly for testing purposes\r\n",
							"dataSchema=\"VendorID STRING,tpep_pickup_datetime TIMESTAMP,tpep_dropoff_datetime TIMESTAMP,passenger_count INT,trip_distance DOUBLE,RatecodeID STRING,store_and_fwd_flag STRING,PULocationID INT,DOLocationID INT,payment_type INT,fare_amount DOUBLE,extra DOUBLE,mta_tax DOUBLE,tip_amount DOUBLE,tolls_amount DOUBLE,improvement_surcharge DOUBLE,total_amount DOUBLE,congestion_surcharge DOUBLE\"\r\n",
							"input=spark.read.csv(path=file,schema=dataSchema,header=True)\r\n",
							"display(input)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"input = input.withColumn(\"sys_date1\",lit(20275)) #Date in Julian Format\r\n",
							"input = input.withColumn(\"sys_date2\",lit(\"2020-10-01\").cast(\"date\")) #Date in Gregorian Format"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(input)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ct=CommonTransforms(input)\r\n",
							"\r\n",
							"# Remove duplicates\r\n",
							"output=ct.deDuplicate()\r\n",
							"\r\n",
							"# Remove duplicates based on key columns\r\n",
							"output=ct.deDuplicate([\"VendorID\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"])\r\n",
							"\r\n",
							"# Remove leading and trailing spaces from all string columns\r\n",
							"output=ct.trim()\r\n",
							"\r\n",
							"# Replace Null Value with generic values\r\n",
							"output = ct.replaceNull(0)\r\n",
							"output = ct.replaceNull(\"NA\")\r\n",
							"output = ct.replaceNull(\"2020-01-01\")\r\n",
							"\r\n",
							"# Replace Null value in Timestamp columns\r\n",
							"output = ct.replaceNull(\"1900-01-01T00:00:00\",\"tpep_pickup_datetime\")\r\n",
							"output = ct.replaceNull(\"9999-12-31T23:59:59\",\"tpep_dropoff_datetime\")\r\n",
							"\r\n",
							"# Replace Null Values with custom defaults\r\n",
							"output = ct.replaceNull({\"passenger_count\":1,\"store_and_fwd_flag\":\"N\",\"tip_amount\":0,\"tolls_amount\":0, \"improvement_surcharge\":0,\"congestion_surcharge\":0})\r\n",
							"\r\n",
							"# Convert UTC timestamps to local\r\n",
							"output = ct.utc_to_local(\"Australia/Sydney\")\r\n",
							"output = ct.utc_to_local(\"Australia/Sydney\",[\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"])\r\n",
							"\r\n",
							"\r\n",
							"# Convert local timestamps to UTC\r\n",
							"output = ct.local_to_utc(\"Australia/Sydney\")\r\n",
							"output = ct.local_to_utc(\"Australia/Sydney\",[\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"])\r\n",
							"\r\n",
							"# Convert time from one Timezone to another\r\n",
							"output = ct.changeTimezone(\"Australia/Sydney\",\"America/New_York\")\r\n",
							"\r\n",
							"# Drop system/non-business columns\r\n",
							"output = ct.dropSysColumns(\"store_and_fwd_flag\")\r\n",
							"\r\n",
							"# Add Checksum\r\n",
							"output = ct.addChecksumCol(\"checksum\")\r\n",
							"\r\n",
							"# Convert Julian date to Calendar date\r\n",
							"output = ct.julian_to_calendar(\"sys_date1\")\r\n",
							"\r\n",
							"# Convert Calendar date to Julian\r\n",
							"output =ct.calendar_to_julian(\"sys_date2\")\r\n",
							"\r\n",
							"# Add literal value columns for e.g audit columns\r\n",
							"audit={\"audit_key\":66363,\"pipeline_id\":\"56f63394bb06dd7f6945f636f1d4018bd50f1850\", \"start_datetime\": \"2020-10-01 10:00:00\", \"end_datetime\": \"2020-10-01 10:02:05\"}\r\n",
							"output = ct.addLitCols(audit)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(output)"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-datalake-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook for unit testing test datalake-functions",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "42832aa4-aa7c-46bf-8637-ceec7724e60e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test Datalake Functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%run /common/datalake-functions"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = readFile(\"bastoragedatalake01\", \"raw-bronze\", \"nyc-yellowtaxi-trip/yellow_tripdata_2020-01.csv\", \",\", True)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"writeFile(df,\"bastoragedatalake01\", \"curated-silver\", \"test/nyc-yellowtaxi-trip/yellow_tripdata_2020-01.parquet\", \"overwrite\")"
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-dedicatedSQL-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook for unit testing dedicatedSQL-functions",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "848d28b9-58a1-416e-b106-7447ffa07a1a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test dedicatedSQL-functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%run /common/dedicatedSQLPool-functions {\"server\": \"ba-synapseanalytics01.sql.azuresynapse.net,1433\",\"database\" :\"dw01\" }"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test readSdpTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df =readSdpTable(\"nyct\",\"nyc_tlc_yellow_trip\",None,None,10)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test insertSdpTable()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Test Overwrite\r\n",
							"schema =\"nyct\"\r\n",
							"table = \"nyc_tlc_yellow_trip_copy\"\r\n",
							"insertSdpTable(df,schema,table,\"overwrite\")\r\n",
							"\r\n",
							"df =readSdpTable(schema,table,None,None,10)\r\n",
							"display(df)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Test Append\r\n",
							"schema =\"nyct\"\r\n",
							"table = \"nyc_tlc_yellow_trip_copy\"\r\n",
							"insertSdpTable(df,schema,table,\"append\")\r\n",
							"\r\n",
							"df =readSdpTable(schema,table,None,None,10)\r\n",
							"display(df)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # Test Error\r\n",
							"# schema =\"nyct\"\r\n",
							"# table = \"nyc_tlc_yellow_trip_copy\"\r\n",
							"# insertSdpTable(df,schema,table,\"error\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Test Ignore\r\n",
							"schema =\"nyct\"\r\n",
							"table = \"nyc_tlc_yellow_trip_copy\"\r\n",
							"insertSdpTable(df,schema,table,\"ignore\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-keyvault-functions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook for unit testing keyvault-functions",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "d39c5cc4-d8d2-42e8-a023-716eddce2d71"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test Key Vault Functions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"Your AKV Linked Service Name\"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"getSecret(\"SecretName\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-pyodbc')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "fc3aa935-849b-45e8-a468-231d79983d1e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/medium31",
						"name": "medium31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable functions for Azure Synapse SQL Dedicated Pool"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"keyvault01\"}"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"server = \"ba-synapseanalytics01.sql.azuresynapse.net,1433\"\r\n",
							"database = \"dw01\""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"uid = getSecret(\"dw01-sqlusername\")\r\n",
							"pwd = getSecret(\"dw01-sqlpassword\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc\r\n",
							"\r\n",
							"cnxn = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + server + \";DATABASE=\" + database + \";UID=\" + uid + \";PWD=\" + pwd )\r\n",
							"cursor = cnxn.cursor()\r\n",
							"cnxn.autocommit = True"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cursor.execute(\"truncate table dbo.DimActors\")\r\n",
							"row = cursor.fetchone()\r\n",
							"if row:\r\n",
							"    print(row)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sql = \"\"\"INSERT INTO stg.nyc_tlc_yellow_trip (vendorID,tpepPickupDateTime,tpepDropoffDateTime,passengerCount,tripDistance,puLocationId,doLocationId,startLon,startLat,endLon,endLat,rateCodeId,storeAndFwdFlag\r\n",
							"      ,paymentType\r\n",
							"      ,fareAmount\r\n",
							"      ,extra\r\n",
							"      ,mtaTax\r\n",
							"      ,improvementSurcharge\r\n",
							"      ,tipAmount\r\n",
							"      ,tollsAmount\r\n",
							"      ,totalAmount)\r\n",
							"SELECT top 100 vendorID\r\n",
							"      ,tpepPickupDateTime\r\n",
							"      ,tpepDropoffDateTime\r\n",
							"      ,passengerCount\r\n",
							"      ,tripDistance\r\n",
							"      ,puLocationId\r\n",
							"      ,doLocationId\r\n",
							"      ,startLon\r\n",
							"      ,startLat\r\n",
							"      ,endLon\r\n",
							"      ,endLat\r\n",
							"      ,rateCodeId\r\n",
							"      ,storeAndFwdFlag\r\n",
							"      ,paymentType\r\n",
							"      ,fareAmount\r\n",
							"      ,extra\r\n",
							"      ,mtaTax\r\n",
							"      ,improvementSurcharge\r\n",
							"      ,tipAmount\r\n",
							"      ,tollsAmount\r\n",
							"      ,totalAmount\r\n",
							"  FROM nyct.nyc_tlc_yellow_trip\r\n",
							"  WHERE NOT EXISTS\r\n",
							"  (SELECT 1 FROM stg.nyc_tlc_yellow_trip\r\n",
							"  where stg.nyc_tlc_yellow_trip.vendorID= nyct.nyc_tlc_yellow_trip.vendorID\r\n",
							"  and stg.nyc_tlc_yellow_trip.tpepPickupDateTime = nyct.nyc_tlc_yellow_trip.tpepPickupDateTime\r\n",
							"  and stg.nyc_tlc_yellow_trip.tpepDropoffDateTime = nyct.nyc_tlc_yellow_trip.tpepDropoffDateTime)\"\"\"\r\n",
							"\r\n",
							"cursor.execute(sql)"
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-upsertSdp-function')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook for unit testing upsertSdp-function",
				"folder": {
					"name": "unit tests"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "10de01ab-203b-47c8-a236-358cb25871b4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Unit Test Upsert Function"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run /common/upsertSdp-function {\"server\": \"ba-synapseanalytics01.sql.azuresynapse.net,1433\",\"database\" :\"dw01\",\"sqlUidSecret\": \"dw01-sqlusername\",\"sqlPwdSecret\":\"dw01-sqlpassword\", \"sinkType\": \"Synapse\"}"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# upsert Synapse Dedicated SQL Pool Table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df =readDspTable(\"nyct\",\"nyc_tlc_yellow_trip\",None,None,10)\r\n",
							"upsertDsp(df,\"nyct\",\"nyc_tlc_yellow_trip_stage\", \"nyct\", \"nyc_tlc_yellow_trip_merge\", [\"vendorID\",\"tpepPickupDateTime\",\"tpepDropoffDateTime\"], DeltaColumn=None) "
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/upsertSdp-function')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Notebook that implements upsert function using PyOdbc",
				"folder": {
					"name": "common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "92669405-9221-48bf-aaae-94da680aa3f9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5a5ba4fb-f4e0-4eaf-a2d3-7db71dfd729d/resourceGroups/rg-dataplatform/providers/Microsoft.Synapse/workspaces/ba-synapseanalytics01/bigDataPools/small31",
						"name": "small31",
						"type": "Spark",
						"endpoint": "https://ba-synapseanalytics01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Re-usable Upsert function for Azure Synapse Analytics SQL Dedicated Pool"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"server=\"<Azure Synapse DedicatedSQLPool>\"\r\n",
							"database = \"<Azure Synapse DedicatedSQLPool/Azure SQL>\"\r\n",
							"sqlUidSecret = \"Azure Key Vault Secret for DedicatedSQLPool user name\"\r\n",
							"sqlPwdSecret = \"Azure Key Vault Secret for DedicatedSQLPool password\""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"%run /common/keyvault-functions {\"kvLinkedService\": \"keyvault01\"}"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/dedicatedSQLPool-functions {\"server\": \"ba-synapseanalytics01.sql.azuresynapse.net,1433\",\"database\" : \"dw01\"}"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /common/dedicatedSQLPool-functions {\"server\": \"ba-synapseanalytics01.sql.azuresynapse.net,1433\",\"database\" :\"dw01\" }"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# upsertSdp()"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upsertSdp(df,SchemaStagingTable,StagingTable, SchemaTargetTable, TargetTable, KeyColumns, DeltaColumn=None) :\r\n",
							"# ##########################################################################################################################  \r\n",
							"# Function: upsertSdp\r\n",
							"# Performs a Merge/Upsert action on a Azure SQL table\r\n",
							"# \r\n",
							"# Parameters:\r\n",
							"# df = Input dataframe\r\n",
							"# StagingTable = Name of Table used to temporarily stage the input data frame\r\n",
							"# SchemaStagingTable = Database schema of staging table\r\n",
							"# SchemaTargetTable = Database schema of target table\r\n",
							"# TargetTable  = Name of Target Table\r\n",
							"# KeyColumns = List of columns that uniquely defines a record in input dataframe\r\n",
							"# DeltaColumn = Name of watermark column in input dataframe\r\n",
							"#\r\n",
							"# Returns:\r\n",
							"# None\r\n",
							"# ##########################################################################################################################  \r\n",
							"    insertSdpTable(df, SchemaStagingTable,StagingTable, \"overwrite\")\r\n",
							"\r\n",
							"    insertCols=\"\"\r\n",
							"    updateCols=\"\"\r\n",
							"    whereCols =\"\"\r\n",
							"    keyCols = \"\"\r\n",
							"\r\n",
							"    # MATCHED Clause of MERGE\r\n",
							"    for keyCol in KeyColumns:\r\n",
							"        keyCols = keyCols + \"source.\" + keyCol + \" = \" + \"target.\" + keyCol + \" and \"\r\n",
							"\r\n",
							"    whereClause = keyCols\r\n",
							"    #Tidy up where clause and remove last \"and\"\r\n",
							"    remove=\"and\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    whereClause = whereClause[::-1].replace(reverse_remove,\"\",1)[::-1]\r\n",
							"\r\n",
							"    # INSERT and UPDATE part of MERGE\r\n",
							"    for col in df.schema.fieldNames():\r\n",
							"        insertCols = insertCols + col + \",\"\r\n",
							"        if col not in KeyColumns: # Exclude identity columns in update statement\r\n",
							"         updateCols =  updateCols + \"target.\" + col + \" = \" + \"source.\" + col + \",\"\r\n",
							"\r\n",
							"    #Tidy up insertCols and remove last \",\"\r\n",
							"    remove=\",\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    insertCols = insertCols[::-1].replace(reverse_remove,\"\",1)[::-1]  \r\n",
							"\r\n",
							"    #Tidy up insertCols and remove last \",\"\r\n",
							"    remove=\",\"\r\n",
							"    reverse_remove=remove[::-1]\r\n",
							"    updateCols = updateCols[::-1].replace(reverse_remove,\"\",1)[::-1]   \r\n",
							"\r\n",
							"    # MERGE Statement in full\r\n",
							"    mergeSQL= \"MERGE INTO \" + SchemaTargetTable +\".\" + TargetTable + \" AS target\" +  \" USING \" + SchemaStagingTable + \".\" + StagingTable + \" AS source\" + \" ON \"\r\n",
							"    mergeSQL = mergeSQL + whereClause\r\n",
							"\r\n",
							"    # WHEN MATCHED\r\n",
							"    if DeltaColumn != None:\r\n",
							"        mergeSQL =  mergeSQL + \" WHEN MATCHED AND \" + \"source.\" + DeltaColumn + \" > \" + \"target.\" + DeltaColumn \r\n",
							"    else:\r\n",
							"        mergeSQL =  mergeSQL + \" WHEN MATCHED\"\r\n",
							"\r\n",
							"    mergeSQL = mergeSQL + \" THEN UPDATE SET \" + updateCols\r\n",
							"\r\n",
							"    # WHEN NOT MATCHED BY TARGET\r\n",
							"    mergeSQL = mergeSQL + \" WHEN NOT MATCHED BY TARGET \" # including BY TARGET clause for Synapse requires the table to be Hash Distributed\r\n",
							"    mergeSQL = mergeSQL + \" THEN INSERT \" +  \"(\"+ insertCols +\")\" + \" VALUES \" + \" ( \"+ insertCols +\" ) \"\r\n",
							"\r\n",
							"    mergeSQL = mergeSQL +\";\"\r\n",
							"    \r\n",
							"    #Execute Merge Statement using Pyodbc\r\n",
							"    uid = getSecret(sqlUidSecret)\r\n",
							"    pwd = getSecret(sqlPwdSecret)\r\n",
							" \r\n",
							"    try:\r\n",
							"        cnxn = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + server + \";DATABASE=\" + database + \";UID=\" + uid + \";PWD=\" + pwd )\r\n",
							"        cursor = cnxn.cursor()\r\n",
							"        cnxn.autocommit = True\r\n",
							"\r\n",
							"        cursor.execute(mergeSQL)\r\n",
							"    except Exception as e:\r\n",
							"        print(\"Upsert to {}.{} failed with exception:\".format(SchemaTargetTable,TargetTable))\r\n",
							"        raise e\r\n",
							"\r\n",
							"    print(\"Upsert statement executed successfully : {} \".format(mergeSQL))\r\n",
							"    return"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/small31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [],
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/medium31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 60
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [],
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dw01')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		}
	]
}